<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Inferential Statistics | CRIM20452 Modelling Criminological Data</title>
  <meta name="description" content="This is a companion workbook for the 2nd year undergraduate module CRIM20452 Modelling Criminological Data at the University of Manchester" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Inferential Statistics | CRIM20452 Modelling Criminological Data" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a companion workbook for the 2nd year undergraduate module CRIM20452 Modelling Criminological Data at the University of Manchester" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Inferential Statistics | CRIM20452 Modelling Criminological Data" />
  
  <meta name="twitter:description" content="This is a companion workbook for the 2nd year undergraduate module CRIM20452 Modelling Criminological Data at the University of Manchester" />
  

<meta name="author" content="Laura Bui &amp; Reka Solymosi" />


<meta name="date" content="2021-04-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="descriptive-statistics.html"/>
<link rel="next" href="hypotheses.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modelling Criminological Data</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="1" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html"><i class="fa fa-check"></i><b>1</b> A First Lesson About R</a><ul>
<li class="chapter" data-level="1.1" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#install-r-r-studio"><i class="fa fa-check"></i><b>1.1</b> Install R &amp; R Studio</a><ul>
<li class="chapter" data-level="1.1.1" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#activity-1-identifying-your-operating-system"><i class="fa fa-check"></i><b>1.1.1</b> Activity 1: Identifying your operating system</a></li>
<li class="chapter" data-level="1.1.2" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#activity-2-install-r-r-studio"><i class="fa fa-check"></i><b>1.1.2</b> Activity 2: Install R &amp; R Studio</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#getting-to-know-rstudio"><i class="fa fa-check"></i><b>1.2</b> Getting to know <code>RStudio</code></a><ul>
<li class="chapter" data-level="1.2.1" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#the-four-panes-of-r-studio"><i class="fa fa-check"></i><b>1.2.1</b> The four panes of R Studio</a></li>
<li class="chapter" data-level="1.2.2" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#customising-r-studio"><i class="fa fa-check"></i><b>1.2.2</b> Customising R Studio</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#todays-3-topics"><i class="fa fa-check"></i><b>1.3</b> Today’s 3 (TOPICS)</a><ul>
<li class="chapter" data-level="1.3.1" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#operators-and-functions"><i class="fa fa-check"></i><b>1.3.1</b> Operators and Functions</a></li>
<li class="chapter" data-level="1.3.2" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#objects"><i class="fa fa-check"></i><b>1.3.2</b> Objects</a></li>
<li class="chapter" data-level="1.3.3" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#packages-1"><i class="fa fa-check"></i><b>1.3.3</b> Packages</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#summary"><i class="fa fa-check"></i><b>1.4</b> SUMMARY</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html"><i class="fa fa-check"></i><b>2</b> Getting to know your data</a><ul>
<li class="chapter" data-level="2.1" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#the-tidyverse"><i class="fa fa-check"></i><b>2.1</b> The Tidyverse</a></li>
<li class="chapter" data-level="2.2" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#r-projects-getting-your-work-files-organised"><i class="fa fa-check"></i><b>2.2</b> R Projects – Getting Your Work Files Organised</a><ul>
<li class="chapter" data-level="2.2.1" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#activity-1-making-yourself-a-project"><i class="fa fa-check"></i><b>2.2.1</b> Activity 1: Making yourself a project</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#importing-data"><i class="fa fa-check"></i><b>2.3</b> Importing Data</a><ul>
<li class="chapter" data-level="2.3.1" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#activity-2-importing-and-viewing-data"><i class="fa fa-check"></i><b>2.3.1</b> Activity 2: Importing and Viewing Data</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#todays-3-topics-1"><i class="fa fa-check"></i><b>2.4</b> Today’s 3 (TOPICS)</a><ul>
<li class="chapter" data-level="2.4.1" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#variables"><i class="fa fa-check"></i><b>2.4.1</b> Variables</a></li>
<li class="chapter" data-level="2.4.2" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#labels"><i class="fa fa-check"></i><b>2.4.2</b> Labels</a></li>
<li class="chapter" data-level="2.4.3" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#subsetting"><i class="fa fa-check"></i><b>2.4.3</b> Subsetting</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#summary-1"><i class="fa fa-check"></i><b>2.5</b> SUMMARY</a><ul>
<li class="chapter" data-level="2.5.1" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#answers-to-activities-if-applicable"><i class="fa fa-check"></i><b>2.5.1</b> Answers to activities (if applicable)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data-visualization.html"><a href="data-visualization.html"><i class="fa fa-check"></i><b>3</b> Data Visualization</a><ul>
<li class="chapter" data-level="3.1" data-path="data-visualization.html"><a href="data-visualization.html#grammar-of-graphics"><i class="fa fa-check"></i><b>3.1</b> Grammar of Graphics</a><ul>
<li class="chapter" data-level="3.1.1" data-path="data-visualization.html"><a href="data-visualization.html#activity-1-getting-ready"><i class="fa fa-check"></i><b>3.1.1</b> Activity 1: Getting Ready</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="data-visualization.html"><a href="data-visualization.html#ggplot2"><i class="fa fa-check"></i><b>3.2</b> ggplot2</a></li>
<li class="chapter" data-level="3.3" data-path="data-visualization.html"><a href="data-visualization.html#todays-3"><i class="fa fa-check"></i><b>3.3</b> Today’s 3</a><ul>
<li class="chapter" data-level="3.3.1" data-path="data-visualization.html"><a href="data-visualization.html#layers"><i class="fa fa-check"></i><b>3.3.1</b> Layers</a></li>
<li class="chapter" data-level="3.3.2" data-path="data-visualization.html"><a href="data-visualization.html#graphs-for-categorical-data"><i class="fa fa-check"></i><b>3.3.2</b> Graphs for Categorical Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="data-visualization.html"><a href="data-visualization.html#graphs-for-numeric-data"><i class="fa fa-check"></i><b>3.3.3</b> Graphs for Numeric Data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="data-visualization.html"><a href="data-visualization.html#summary-2"><i class="fa fa-check"></i><b>3.4</b> SUMMARY</a><ul>
<li class="chapter" data-level="3.4.1" data-path="data-visualization.html"><a href="data-visualization.html#answers-to-activities"><i class="fa fa-check"></i><b>3.4.1</b> ANSWERS TO ACTIVITIES:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html"><i class="fa fa-check"></i><b>4</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="4.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#revisiting-descriptive-statistics"><i class="fa fa-check"></i><b>4.1</b> Revisiting Descriptive Statistics</a><ul>
<li class="chapter" data-level="4.1.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#activity-1-our-preparation-routine"><i class="fa fa-check"></i><b>4.1.1</b> Activity 1: Our preparation routine</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#todays-3-1"><i class="fa fa-check"></i><b>4.2</b> Today’s 3</a><ul>
<li class="chapter" data-level="4.2.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>4.2.1</b> Central Tendency</a></li>
<li class="chapter" data-level="4.2.2" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#outliers"><i class="fa fa-check"></i><b>4.2.2</b> Outliers</a></li>
<li class="chapter" data-level="4.2.3" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#measures-of-dispersion"><i class="fa fa-check"></i><b>4.2.3</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#summary-3"><i class="fa fa-check"></i><b>4.3</b> SUMMARY</a><ul>
<li class="chapter" data-level="4.3.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#answers-to-activities-where-applicable"><i class="fa fa-check"></i><b>4.3.1</b> Answers to Activities (where applicable)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="inferential-statistics.html"><a href="inferential-statistics.html"><i class="fa fa-check"></i><b>5</b> Inferential Statistics</a><ul>
<li class="chapter" data-level="5.1" data-path="inferential-statistics.html"><a href="inferential-statistics.html#generalising-about-the-world-from-data"><i class="fa fa-check"></i><b>5.1</b> Generalising About the World from Data</a><ul>
<li class="chapter" data-level="5.1.1" data-path="inferential-statistics.html"><a href="inferential-statistics.html#activity-1-our-preparation-routine-1"><i class="fa fa-check"></i><b>5.1.1</b> Activity 1: Our preparation routine</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="inferential-statistics.html"><a href="inferential-statistics.html#todays-3-2"><i class="fa fa-check"></i><b>5.2</b> Today’s 3</a><ul>
<li class="chapter" data-level="5.2.1" data-path="inferential-statistics.html"><a href="inferential-statistics.html#samples"><i class="fa fa-check"></i><b>5.2.1</b> Samples</a></li>
<li class="chapter" data-level="5.2.2" data-path="inferential-statistics.html"><a href="inferential-statistics.html#the-standard-error"><i class="fa fa-check"></i><b>5.2.2</b> The Standard Error</a></li>
<li class="chapter" data-level="5.2.3" data-path="inferential-statistics.html"><a href="inferential-statistics.html#confidence-intervals"><i class="fa fa-check"></i><b>5.2.3</b> Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="inferential-statistics.html"><a href="inferential-statistics.html#summary-4"><i class="fa fa-check"></i><b>5.3</b> SUMMARY</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="hypotheses.html"><a href="hypotheses.html"><i class="fa fa-check"></i><b>6</b> Hypotheses</a><ul>
<li class="chapter" data-level="6.1" data-path="hypotheses.html"><a href="hypotheses.html#hypothesis-testing"><i class="fa fa-check"></i><b>6.1</b> Hypothesis Testing</a><ul>
<li class="chapter" data-level="6.1.1" data-path="hypotheses.html"><a href="hypotheses.html#the-null-hypothesis"><i class="fa fa-check"></i><b>6.1.1</b> The null hypothesis</a></li>
<li class="chapter" data-level="6.1.2" data-path="hypotheses.html"><a href="hypotheses.html#directional-hypotheses"><i class="fa fa-check"></i><b>6.1.2</b> Directional hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="hypotheses.html"><a href="hypotheses.html#todays-3-3"><i class="fa fa-check"></i><b>6.2</b> Today’s 3</a><ul>
<li class="chapter" data-level="6.2.1" data-path="hypotheses.html"><a href="hypotheses.html#statistical-significance"><i class="fa fa-check"></i><b>6.2.1</b> Statistical Significance</a></li>
<li class="chapter" data-level="6.2.2" data-path="hypotheses.html"><a href="hypotheses.html#hypothesis-tests-for-the-binomial-distribution"><i class="fa fa-check"></i><b>6.2.2</b> Hypothesis Tests for the Binomial Distribution</a></li>
<li class="chapter" data-level="6.2.3" data-path="hypotheses.html"><a href="hypotheses.html#hypothesis-tests-for-the-normal-distribution"><i class="fa fa-check"></i><b>6.2.3</b> Hypothesis Tests for the Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="hypotheses.html"><a href="hypotheses.html#summary-5"><i class="fa fa-check"></i><b>6.3</b> SUMMARY</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="relationships-with-categorical-variables.html"><a href="relationships-with-categorical-variables.html"><i class="fa fa-check"></i><b>7</b> Relationships with Categorical Variables</a><ul>
<li class="chapter" data-level="7.1" data-path="relationships-with-categorical-variables.html"><a href="relationships-with-categorical-variables.html#associating-with-categorical-variables"><i class="fa fa-check"></i><b>7.1</b> Associating with Categorical Variables</a><ul>
<li class="chapter" data-level="7.1.1" data-path="relationships-with-categorical-variables.html"><a href="relationships-with-categorical-variables.html#activity-1-our-r-prep-routine"><i class="fa fa-check"></i><b>7.1.1</b> Activity 1: Our <code>R</code> prep routine</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="relationships-with-categorical-variables.html"><a href="relationships-with-categorical-variables.html#todays-3-4"><i class="fa fa-check"></i><b>7.2</b> Today’s 3</a><ul>
<li class="chapter" data-level="7.2.1" data-path="relationships-with-categorical-variables.html"><a href="relationships-with-categorical-variables.html#independent-and-dependent-variables"><i class="fa fa-check"></i><b>7.2.1</b> Independent and Dependent Variables</a></li>
<li class="chapter" data-level="7.2.2" data-path="relationships-with-categorical-variables.html"><a href="relationships-with-categorical-variables.html#comparing-means-the-t-test"><i class="fa fa-check"></i><b>7.2.2</b> Comparing means: the t-test</a></li>
<li class="chapter" data-level="7.2.3" data-path="relationships-with-categorical-variables.html"><a href="relationships-with-categorical-variables.html#chi-square"><i class="fa fa-check"></i><b>7.2.3</b> Chi-square</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="relationships-with-categorical-variables.html"><a href="relationships-with-categorical-variables.html#summary-6"><i class="fa fa-check"></i><b>7.3</b> SUMMARY</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="more-on-effect-sizes.html"><a href="more-on-effect-sizes.html"><i class="fa fa-check"></i><b>8</b> (More) On Effect Sizes</a><ul>
<li class="chapter" data-level="8.1" data-path="more-on-effect-sizes.html"><a href="more-on-effect-sizes.html#measures-of-association-a-continuation"><i class="fa fa-check"></i><b>8.1</b> Measures of Association: A Continuation</a><ul>
<li class="chapter" data-level="8.1.1" data-path="more-on-effect-sizes.html"><a href="more-on-effect-sizes.html#activity-1-the-r-prep-routine"><i class="fa fa-check"></i><b>8.1.1</b> Activity 1: The <code>R</code> prep routine</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="more-on-effect-sizes.html"><a href="more-on-effect-sizes.html#todays-2"><i class="fa fa-check"></i><b>8.2</b> Today’s 2 (!)</a><ul>
<li class="chapter" data-level="8.2.1" data-path="more-on-effect-sizes.html"><a href="more-on-effect-sizes.html#correlations"><i class="fa fa-check"></i><b>8.2.1</b> Correlations</a></li>
<li class="chapter" data-level="8.2.2" data-path="more-on-effect-sizes.html"><a href="more-on-effect-sizes.html#power-analysis"><i class="fa fa-check"></i><b>8.2.2</b> Power analysis</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="more-on-effect-sizes.html"><a href="more-on-effect-sizes.html#summary-7"><i class="fa fa-check"></i><b>8.3</b> SUMMARY</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>9</b> Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="regression.html"><a href="regression.html#multiple-and-simultaneous-relationships"><i class="fa fa-check"></i><b>9.1</b> Multiple and Simultaneous Relationships</a></li>
<li class="chapter" data-level="9.2" data-path="regression.html"><a href="regression.html#todays-3-5"><i class="fa fa-check"></i><b>9.2</b> Today’s 3</a><ul>
<li class="chapter" data-level="9.2.1" data-path="regression.html"><a href="regression.html#motivating-regression"><i class="fa fa-check"></i><b>9.2.1</b> Motivating Regression</a></li>
<li class="chapter" data-level="9.2.2" data-path="regression.html"><a href="regression.html#interpreting-ols-regression"><i class="fa fa-check"></i><b>9.2.2</b> Interpreting OLS Regression</a></li>
<li class="chapter" data-level="9.2.3" data-path="regression.html"><a href="regression.html#logistic-regression"><i class="fa fa-check"></i><b>9.2.3</b> Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="regression.html"><a href="regression.html#summary-8"><i class="fa fa-check"></i><b>9.3</b> SUMMARY</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">CRIM20452 Modelling Criminological Data</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="inferential-statistics" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Inferential Statistics</h1>
<div id="samples-standard-errors-and-confidence-intervals" class="section level4 unnumbered">
<h4><em>Samples, Standard Errors, and Confidence Intervals</em></h4>
</div>
<div id="learning-outcomes-4" class="section level4 unnumbered">
<h4><strong>Learning Outcomes:</strong></h4>
<ul>
<li>Understand what inferential statistics are and why they are used</li>
<li>Learn how samples can be used to draw conclusions about the population</li>
<li>Learn about standard errors and confidence intervals and how to calculate them</li>
</ul>
</div>
<div id="todays-learning-tools-4" class="section level4 unnumbered">
<h4><strong>Today’s Learning Tools:</strong></h4>
<div id="total-number-of-activities-8-1" class="section level5 unnumbered">
<h5><em>Total number of activities</em>: 8</h5>
</div>
<div id="data-4" class="section level5 unnumbered">
<h5><em>Data:</em></h5>
<ul>
<li>Synthetic data we make ourselves</li>
</ul>
</div>
<div id="packages-5" class="section level5 unnumbered">
<h5><em>Packages:</em></h5>
<ul>
<li><code>dplyr</code></li>
<li><code>ggplot2</code></li>
<li><code>mosaic</code></li>
</ul>
</div>
<div id="functions-introduced-and-packages-to-which-they-belong-4" class="section level5 unnumbered">
<h5><em>Functions introduced (and packages to which they belong)</em></h5>
<ul>
<li><code>bind_rows()</code> : Combine data frame(s) together row-wise (<code>dplyr</code>)</li>
<li><code>do()</code> : Loop for resampling (<code>mosaic</code>)</li>
<li><code>geom_density()</code> : Geometry layer for density plots (<code>ggplot2</code>)</li>
<li><code>geom_errorbarh()</code> : Draws horizontal error bars by specifying maximum and minimum value (<code>ggplot2</code>)</li>
<li><code>geom_vline()</code> : Geometry layer for adding vertical lines (<code>ggplot2</code>)</li>
<li><code>if_else()</code> : Tests conditions for true or false, taking on values for each (<code>dplyr</code>)</li>
<li><code>rnorm()</code> : Create synthetic normally distributed data (<code>base R</code>)</li>
<li><code>round()</code> : Rounds to nearest whole number or specified number of decimals (<code>base R</code>)</li>
<li><code>sample()</code> : Randomly sample from a vector or data frame (<code>mosaic</code>)</li>
<li><code>set.seed()</code> : Random number generator start point (<code>base R</code>)</li>
</ul>
<p><br>
<br></p>
<hr />
</div>
</div>
<div id="generalising-about-the-world-from-data" class="section level2">
<h2><span class="header-section-number">5.1</span> Generalising About the World from Data</h2>
<p>Last time, we revisited a familiar sort of statistics: descriptive. But we also learned how to conduct these statistics using <code>R</code>. Today, we learn the other main branch of statistics: inferential.</p>
<p>Whereas descriptive statistics are concerned with summarising and describing your data, <strong>inferential (or frequentist) statistics</strong> are concerned with using the data to say something about the world in which we live. For example, we can make conclusions on body worn camera use in agencies across the country as a whole from data on only a handful of agencies. Using samples drawn from our population of interest, we can conduct statistical analyses to generalise to our lives and what we observe around us.</p>
<p>Inferences made from inferential statistics are not bound to one dataset and sample, and that is the strength of this type of statistics. It is able to <em>generalise</em>, like in the previous example on body worn cameras. Because, however, we will be saying something that is applicable to the ‘real’ world, we must understand the theory for which makes this possible.</p>
<p>Today’s learning experience is the most theoretical of this course unit. To understand later inferential statistical analyses is to first understand the base on which they stand.</p>
<p><br></p>
<div id="activity-1-our-preparation-routine-1" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Activity 1: Our preparation routine</h3>
<p>As usual, we begin by opening your existing <code>R</code> project, then installing (if need) and loading the required packages listed above under the ‘Packages’ subheading.</p>
<p><br>
<br></p>
<hr />
</div>
</div>
<div id="todays-3-2" class="section level2">
<h2><span class="header-section-number">5.2</span> Today’s 3</h2>
<p>Our three substantive topics today are: <strong>samples</strong>, <strong>standard errors</strong>, and <strong>confidence intervals</strong>. As you continue the remainder of this course unit, you will observe how important it is to collect accurate information to conduct inferential statistics. Your findings and conclusions are only as good as their basis, and if that basis is a shoddy collection of data, what you have to say will reflect that. An important way to collect accurate information is to ensure that what we have is representative of that real world. This is where samples arrive to play.</p>
<p><br>
<br></p>
<hr />
<div id="samples" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Samples</h3>
<p>Say we are curious about how widespread robbery in England and Wales has been in the past 12 months. We could obtain police-recorded data to tell us this information. We also, however, know from previous criminology classes that many people do not report crimes to the police, so this data is limited, unable to tap into what is known as ‘the dark figure of crime’.</p>
<p>One way to address this limitation is with self-report victimisation surveys, such as the <a href="https://www.crimesurvey.co.uk/en/index.html">Crime Survey for England and Wales</a>. It would be ideal to survey everyone in England and Wales about whether they have been a victim of robbery in the past year and how many times they have been robbed. Surveying the entire <strong>population</strong>, however, is impractical because of time and financial constraints. So, although the Crime Survey addresses the limitation of police-recorded data, it is still unable to obtain information on the entire population of interest – everyone living in England and Wales.</p>
<p>Sure, eventually mass collection of data from the population may be possible in the future as we have glimpsed with the sheer amount gathered by social media corporations, but even then, access and availability remain issues. Because of these problems in collecting information from the population, we use a sample.</p>
<p>A <strong>sample</strong> is a small selection of that population of interest. You may recall from last semester and from your first year research methods classes the different approaches to sampling – different ways to select individuals to form your sample. Examples are random sampling and convenience sampling. The aim of good sampling is to acheive <strong>external validity</strong>.</p>
<p>If our sample has high external validity, it means that our sample is representative of our population of interest. Therefore, we can be confident that whatever we say and conclude about our sample can be generalised to the population. That is what the Crime Survey of England and Wales does: sample from the population to get an estimate of how widespread crime and victimisation are in the whole of the population.</p>
<!-- The big concern now is: how do we know that a sample is generalisable to the wider population? Is there a way to prove that? -->
<p><br></p>
<div id="how-samples-are-representative-of-the-population" class="section level4">
<h4><span class="header-section-number">5.2.1.1</span> How samples are representative of the population</h4>
<p>How do we know that a sample is good at representing the population of interest? In the real world, as we have learned, it is often impossible to get data from the whole population. To illustrate how we can trust our statistics from our sample to represent the estimates in the population from which they are derived, also known as <strong>parameters</strong>, we will create a fake population from which we draw samples using <strong>synthetic data</strong>.</p>
<!-- We create **synthetic data** to represent this fake population to demonstrate how it is possible for a sample to be used to estimate what goes on in the whole population. 
Why the data are based on a fake population is because rarely do we have information on the whole population, of course.  -->
<p>Last time, we learned about distributions too. Specifically, we focused on the normal distribution. This is also called a bell curve, because when we squint a little, the shape looks like a bell. Remember that normal distributions are symmetrical (there is no skew) and the mean is the same as the median. The below visual is that very distribution. It also depicts a nifty fact of every normal distribution called the <strong>68-95-99.7 rule</strong>, which we will learn more about in this lesson. (It will be first introduced in section 5.2.1.5 .)</p>
<!--In addition, there were measures of distribution, or dispersion. For example, the standard deviation. Later we learn more about a nifty fact on the normal distribution. For now, the normal distribution below illustrates this nifty fact: 68% of your data will fall within +/- 1 standard deviation of your mean; 95% of your data within +/- 2 standard deviations of your mean; and 99% of your data within +/- 3 standard deviations of your mean.-->
<p><br>
<br></p>
<p><img src="05-inferential-statistics_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p><br>
<br></p>
<p>Much of what we will be learning in the coming weeks will make the assumption that our data are normally distributed. Of course, there will be exceptions. This is something you will need to check for before embarking on further statistical analyses. For today, though, we focus on the normal distribution and we create synthetic data on intelligence quotient (IQ) scores of American probationers to show how a sample can be representative of the population.</p>
</div>
<div id="activity-2-making-normally-distributed-synthetic-data" class="section level4">
<h4><span class="header-section-number">5.2.1.2</span> Activity 2: Making normally distributed synthetic data</h4>
<p>The synthetic data will consist of randomly generated numbers to represent the intelligence quotient (IQ) scores of every probationer in the US, which is a population of about 3.6 million. For this example, we assume the mean IQ scores to be 100 and the standard deviation to be 15.</p>
<p>We create this population distribution by using the function <code>nrnorm()</code> and assigning this to a vector object called <code>prob_iq</code>:
<br></p>
<div class="sourceCode" id="cb235"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb235-1" title="1"><span class="co"># Inside nrnorm(), we specify the parameters:</span></a>
<a class="sourceLine" id="cb235-2" title="2"><span class="co"># n = number of observations, which is 3.6 milion -- our population of US probationers</span></a>
<a class="sourceLine" id="cb235-3" title="3"><span class="co"># mean = IQ score of 100</span></a>
<a class="sourceLine" id="cb235-4" title="4"><span class="co"># sd = dispersion of 15 IQ points around the mean</span></a>
<a class="sourceLine" id="cb235-5" title="5"></a>
<a class="sourceLine" id="cb235-6" title="6">prob_iq &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">3600000</span>, <span class="dt">mean =</span> <span class="dv">100</span>, <span class="dt">sd =</span> <span class="dv">15</span>)</a></code></pre></div>
<p><br></p>
<p>We now have a vector of numbers, all randomly created. Let us get some descriptive statistics:
<br></p>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb236-1" title="1"><span class="kw">mean</span>(prob_iq) </a></code></pre></div>
<pre><code>## [1] 100.001</code></pre>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb238-1" title="1"><span class="kw">median</span>(prob_iq) </a></code></pre></div>
<pre><code>## [1] 99.9976</code></pre>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb240-1" title="1"><span class="kw">sd</span>(prob_iq) </a></code></pre></div>
<pre><code>## [1] 14.99885</code></pre>
<p><br></p>
<p>By obtaining some descriptive statistics, we are verifying that the mean and SD are indeed 100 and 15, but you may notice a few discrepancies:</p>
<ul>
<li><ol style="list-style-type: decimal">
<li>The obtained mean is not <em>exactly</em> 100 and the SD is not <em>exactly</em> 15, although they are very close to those parameters (population estimates).</li>
</ol></li>
<li><ol start="2" style="list-style-type: decimal">
<li>Your values obtained in your <code>RStudio</code> may be slightly different to that of the lab notes. The reason is <code>R</code> <em>randomly</em> generates these numbers for you. If you re-run the code with <code>rnorm</code> above to re-create your <code>prob_iq</code>, you will get another set of numbers!</li>
</ol></li>
</ul>
<p><br></p>
<p>To ensure we have the same set of numbers, we can set a specific seed from which the random numbers should ‘grow’. If we choose the same seed, then we will get the same set of random numbers and, thus, the same results.</p>
<p>We set a seed using the function <code>set.seed()</code>, which ensures it generates the exact same distribution for this session. Then we re-create the <code>prob_iq</code> object:
<br></p>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb242-1" title="1"><span class="kw">set.seed</span>(<span class="dv">1612</span>) <span class="co"># Use this number! </span></a>
<a class="sourceLine" id="cb242-2" title="2"></a>
<a class="sourceLine" id="cb242-3" title="3">prob_iq &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">3600000</span>, <span class="dt">mean =</span> <span class="dv">100</span>, <span class="dt">sd =</span> <span class="dv">15</span>)</a></code></pre></div>
<p><br></p>
<p>Again, obtain the mean, median, and the SD to verify that we have the same results:
<br></p>
<div class="sourceCode" id="cb243"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb243-1" title="1"><span class="kw">mean</span>(prob_iq) </a></code></pre></div>
<pre><code>## [1] 100.0032</code></pre>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb245-1" title="1"><span class="kw">median</span>(prob_iq) </a></code></pre></div>
<pre><code>## [1] 100.0035</code></pre>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb247-1" title="1"><span class="kw">sd</span>(prob_iq) </a></code></pre></div>
<pre><code>## [1] 14.99679</code></pre>
<p><br></p>
<p>The values, though slightly different from those stated in our <code>rnorm()</code> code, should match those in our lab notes.</p>
<p>With this set of randomly generated IQ scores, let us build a data frame using the function <code>data.frame()</code> in which we will create two columns: one for a unique identifier for each probationer called <code>probationer_id</code> and another for IQ scores called <code>IQ</code>. The data frame object will be called <code>prob_off</code>:
<br></p>
<div class="sourceCode" id="cb249"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb249-1" title="1">prob_off &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">probationer_id =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">3600000</span>,      <span class="co"># create a column whose numbers range from 1 to 3.6 million</span></a>
<a class="sourceLine" id="cb249-2" title="2">                       <span class="dt">IQ =</span> prob_iq )     <span class="co"># create a column whose numbers are the random IQ scores</span></a></code></pre></div>
<p><br></p>
<p>We need to take one last step to complete our fake population data: the values for IQ score must be whole numbers (integers). To achieve this, we will use the <code>round()</code> function:
<br></p>
<div class="sourceCode" id="cb250"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb250-1" title="1"><span class="co"># Here, we tell R that we want to round the numbers in our variable IQ</span></a>
<a class="sourceLine" id="cb250-2" title="2"><span class="co"># We specify 0 after our variable to tell R the number of decimal places we want displayed - we want zero decimal places because we want integers</span></a>
<a class="sourceLine" id="cb250-3" title="3"></a>
<a class="sourceLine" id="cb250-4" title="4">prob_off<span class="op">$</span>IQ &lt;-<span class="st"> </span><span class="kw">round</span>(prob_off<span class="op">$</span>IQ, <span class="dv">0</span>)</a></code></pre></div>
<p><br></p>
<p>Our data of a fake population of 3.6 million US probationers and their IQ scores are complete. You can have a look at this data with the <code>View()</code> function. In addition, visualising this distribution will help us identify its shape. We use <code>ggplot2</code>:
<br></p>
<div class="sourceCode" id="cb251"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb251-1" title="1"><span class="kw">View</span>(prob_off)</a></code></pre></div>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb252-1" title="1"><span class="co"># Double check that &#39;ggplot2&#39; is loaded</span></a>
<a class="sourceLine" id="cb252-2" title="2"><span class="kw">ggplot</span>(prob_off) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb252-3" title="3"><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x =</span> IQ), <span class="dt">bins =</span> <span class="dv">60</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb252-4" title="4"><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(prob_off<span class="op">$</span>IQ), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="co"># We add a red line in the code to show the mean of the population IQ</span></a></code></pre></div>
<p><img src="05-inferential-statistics_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p><br></p>
<p>From the histogram, we observe that the scores are normally distributed, which is bell-shaped, and there is no skewness on other side. The majority of probationers have an IQ around the mean of 100.</p>
<p><br>
<br></p>
<hr />
</div>
<div id="activity-3-taking-a-sample-from-our-fake-population" class="section level4">
<h4><span class="header-section-number">5.2.1.3</span> Activity 3: Taking a sample from our fake population</h4>
<p>From the above, we created a dataset of all 3.6 million probationers in the US. This is our hypothetical population. When we look at the mean, median, and standard deviation of this population, these numbers are considered the <em>true</em> estimates of IQ scores in the population of American probationers.</p>
<p>If we now take a sample from this population, how accurate would our sample estimates be compared to the population estimates?</p>
<p>First, we draw a sample from this population. We use the function <code>sample()</code> from the <code>mosaic</code> package to get this random sample of 100 probationers:
<br></p>
<div class="sourceCode" id="cb253"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb253-1" title="1"><span class="co"># &#39;size&#39; specifies the size of the sample to take</span></a>
<a class="sourceLine" id="cb253-2" title="2"><span class="co"># &#39;x&#39; specifies what dataset to sample from</span></a>
<a class="sourceLine" id="cb253-3" title="3"><span class="co"># We take a sample of 100 from our data frame, ‘prob_off’, and put it into an object called ‘sample1’</span></a>
<a class="sourceLine" id="cb253-4" title="4"></a>
<a class="sourceLine" id="cb253-5" title="5">sample1 &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> prob_off, <span class="dt">size =</span> <span class="dv">100</span>)</a></code></pre></div>
<p><br></p>
<p>We then obtain some descriptive statistics of our newly obtained sample called <code>sample1</code>:
<br></p>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb254-1" title="1"><span class="kw">mean</span>(sample1<span class="op">$</span>IQ) <span class="co"># 101.59 </span></a></code></pre></div>
<pre><code>## [1] 101.59</code></pre>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb256-1" title="1"><span class="kw">median</span>(sample1<span class="op">$</span>IQ) <span class="co"># 101.5 </span></a></code></pre></div>
<pre><code>## [1] 101.5</code></pre>
<div class="sourceCode" id="cb258"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb258-1" title="1"><span class="kw">sd</span>(sample1<span class="op">$</span>IQ) <span class="co"># 16.27485</span></a></code></pre></div>
<pre><code>## [1] 16.27485</code></pre>
<p><br></p>
<p>The sample estimates seem very close to the ‘true’ estimates, or parameters, from our population of probationers. Let us obtain another sample using another seed:
<br></p>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb260-1" title="1"><span class="kw">set.seed</span>(<span class="dv">90201</span>)</a>
<a class="sourceLine" id="cb260-2" title="2">sample2 &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> prob_off, <span class="dt">size =</span> <span class="dv">100</span>)</a></code></pre></div>
<p><br></p>
<p>We again obtain descriptive statistics but, this time, of our second generated sample, <code>sample2</code>:
<br></p>
<div class="sourceCode" id="cb261"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb261-1" title="1"><span class="kw">mean</span>(sample2<span class="op">$</span>IQ) <span class="co"># 100.81 </span></a></code></pre></div>
<pre><code>## [1] 100.81</code></pre>
<div class="sourceCode" id="cb263"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb263-1" title="1"><span class="kw">median</span>(sample2<span class="op">$</span>IQ) <span class="co"># 102.5 </span></a></code></pre></div>
<pre><code>## [1] 102.5</code></pre>
<div class="sourceCode" id="cb265"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb265-1" title="1"><span class="kw">sd</span>(sample2<span class="op">$</span>IQ) <span class="co"># 15.08206</span></a></code></pre></div>
<pre><code>## [1] 15.08206</code></pre>
<p><br></p>
<p>These are slightly different estimates from those obtained in <code>sample1</code>. This is because we drew a different sample from our population. The estimates from <code>sample2</code> are still close to our population estimates (M = 100, SD = 15). We now draw a third sample and obtain some descriptive statistics of it:
<br></p>
<div class="sourceCode" id="cb267"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb267-1" title="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb267-2" title="2">sample3 &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> prob_off, <span class="dt">size =</span> <span class="dv">100</span>)</a>
<a class="sourceLine" id="cb267-3" title="3"></a>
<a class="sourceLine" id="cb267-4" title="4"><span class="kw">mean</span>(sample3<span class="op">$</span>IQ) <span class="co"># 99.41 </span></a></code></pre></div>
<pre><code>## [1] 99.41</code></pre>
<div class="sourceCode" id="cb269"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb269-1" title="1"><span class="kw">median</span>(sample3<span class="op">$</span>IQ) <span class="co"># 101 </span></a></code></pre></div>
<pre><code>## [1] 101</code></pre>
<div class="sourceCode" id="cb271"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb271-1" title="1"><span class="kw">sd</span>(sample3<span class="op">$</span>IQ) <span class="co"># 15.53497</span></a></code></pre></div>
<pre><code>## [1] 15.53497</code></pre>
<p><br></p>
<p>Again, we get slightly different estimates from those in the previous samples because we drew another different sample from the population. Depending on which sample we drew, our estimates would be different. This variation in estimates is known as <strong>sampling variability</strong>, an unavoidable consequence of randomly sampling observations from the population.</p>
<p>For example, above, we took three different samples. Let us look at the mean of each sample again:
<br></p>
<div class="sourceCode" id="cb273"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb273-1" title="1"><span class="kw">mean</span>(sample1<span class="op">$</span>IQ) </a></code></pre></div>
<pre><code>## [1] 101.59</code></pre>
<div class="sourceCode" id="cb275"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb275-1" title="1"><span class="kw">mean</span>(sample2<span class="op">$</span>IQ) </a></code></pre></div>
<pre><code>## [1] 100.81</code></pre>
<div class="sourceCode" id="cb277"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb277-1" title="1"><span class="kw">mean</span>(sample3<span class="op">$</span>IQ) </a></code></pre></div>
<pre><code>## [1] 99.41</code></pre>
<p><br></p>
<p>Drawing three different samples from the population by setting different seeds illustrates the knowledge that we will get slightly different values for the mean for each sample we create. These mean values, though, often will be close to the true population mean. This gives us some confidence that our sample may be representative of the population, but it is the sampling distribution that increases that confidence.</p>
<p><br>
<br></p>
<hr />
</div>
<div id="the-sampling-distribution" class="section level4">
<h4><span class="header-section-number">5.2.1.4</span> The Sampling Distribution</h4>
<p>Getting different estimates each time we randomly sample from the population, however, is a real problem facing researchers: every time we take a sample from our population of interest, we need to be confident that its estimates are similar to the true but unknown estimates of that population.</p>
<p>Sampling variability makes up what is known as the <strong>sampling distribution</strong>. This distribution comprises the values of a certain statistic of the many samples we draw from the same population. All these values, when visualised together, will follow a normal distribution whose total average will reflect that true population value.</p>
<p>Take our three samples we created previously as an example. We had compared their means and found that they slightly differed from each other. If we were to draw more samples (sample4, sample5, sample6, and so on) from the population, we would find that our resampling – taking repeated samples from the same population of interest – results in creating many (sample) means too. The interesting bit is that when you take the overall average of all the means you obtained from those many samples, it is very close to that true population mean of 100. This is what is known specifically as the sampling distribution of the mean.</p>
<p><br></p>
</div>
<div id="activity-4-the-sampling-distribution-of-our-probationers" class="section level4">
<h4><span class="header-section-number">5.2.1.5</span> Activity 4: The sampling distribution of our probationers</h4>
<p>Let us demonstrate the sampling distribution by taking 1,000 samples of 100 people each from our population of 3.6 million probationers. Imagine we received funding to administer general intelligence tests. Each time, we take a random sample of 100 probationers, administer each individual in that sample a general intelligence test, and take the mean IQ of that sample. These mean IQs from each sample are the sample means.</p>
<p>Before, we had used the function <code>sample()</code> to obtain one sample of 100 people (<code>sample(x = prob_off, size = 100)</code>). We do something similar except we repeat this 1,000 times by using the <code>do()</code> function, which tells <code>R</code> to do something repeatedly:
<br></p>
<div class="sourceCode" id="cb279"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb279-1" title="1"><span class="co"># In other words, to take 1,000 samples with 100 probationers each, we want to &#39;do&#39; `sample(x = prob_off, size = 100)` 1,000 times and call it &#39;sample1000&#39;: </span></a>
<a class="sourceLine" id="cb279-2" title="2"></a>
<a class="sourceLine" id="cb279-3" title="3">sample1000 &lt;-<span class="st"> </span><span class="kw">do</span>(<span class="dv">1000</span>) <span class="op">*</span><span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> prob_off, <span class="dt">size =</span> <span class="dv">100</span>)</a></code></pre></div>
<p><br></p>
<p>This might take a while (you are sampling 1,000 times after all), so give <code>R</code> time to process. A little stop sign icon may appear in the corner of the console to tell you to be patient. When done, you will see the <code>sample1000</code> object appear in your environment. Get to know this new data with <code>View()</code>.</p>
<p>You can see we have the variables <code>probationer_id</code> (prisoner ID) and <code>IQ</code> (IQ score), but we also have these new variables, <code>.row</code> and <code>.index</code>. The variable <code>.row</code> refers to the probationer’s position in their particular sample, while <code>.index</code> refers to the sample to which they belong. For example, if <code>.index</code> indicates ‘1’ for a particular observation, it means belonging to the first sample; if ‘2’, the second sample, and so on up to 1,000.</p>
<!--To select a particular sample, you could use the `filter()` function. For example, to select only the second sample, whereby all cases are `.index == 2`. But we want to keep all 1,000 samples. Instead, what -->
<p>We now want the <em>average IQ for each sample</em>. You may recall how to get the mean for each category of a categorical variable from last week - we used the functions <code>group_by()</code> and <code>summarise()</code> (see Lesson 4, section 4.2.1.3). Let us use these again to create the new object called <code>sample_means1000</code>, which contains the mean IQ for each sample, all 1,000 of them:
<br></p>
<div class="sourceCode" id="cb280"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb280-1" title="1">sample_means1000 &lt;-<span class="st"> </span>sample1000 <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb280-2" title="2"><span class="st">  </span><span class="kw">group_by</span>(.index) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># Group by .index  (the sample id)</span></a>
<a class="sourceLine" id="cb280-3" title="3"><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">meanIQ =</span> <span class="kw">mean</span>(IQ)) <span class="co"># Creating new variable of mean IQ</span></a></code></pre></div>
<pre><code>## `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<p><br></p>
<p>The resulting data frame,<code>sample_means1000</code>, has two columns: one for sample id and one for the mean score of IQ for that specific sample. It has 1,000 observations - one for each sample.</p>
<p>What does our sampling variability look like in <code>sample_means1000</code>? We can visualise this sampling distribution to compare with the population distribution:
<br></p>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb282-1" title="1"><span class="kw">ggplot</span>(<span class="dt">data =</span> sample_means1000) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb282-2" title="2"><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x =</span> meanIQ)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb282-3" title="3"><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(meanIQ)), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>)</a></code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="05-inferential-statistics_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p><br></p>
<p>The histogram of our sampling distribution shows this very important concept in inferential statistics: whenever we draw a sample, sometimes we <em>overestimate</em> the parameter and sometimes we <em>underestimate</em> the parameter. This means that sometimes our sample mean is <em>larger</em> than our population mean and, in other times, it is <em>smaller</em> than the population mean.</p>
<p>Usually the mean of the randomly selected sample will fall close to the populatin mean, but occasionally, it will fall far from it. What is exciting is that if you randomly draw repeated samples from the same population and calculate the mean of each sample, then plot the frequency of those means, you will get the <em>normal distribution</em>. It does not even matter if your data are normally distributed - your sample statistics, including the mean, will be!</p>
<p>According to our sampling distribution of probationer IQ scores, drawing a sample with a mean IQ score that is radically different from that of the population would be unlikely. From the histogram, observe how 68% of observations in a normal distribution fall within one standard deviation above and below the mean and 95% within two standard deviations above and below the mean. This should be reassuring. This is the <strong>68-95-99.7 rule</strong>, or known as an empirical rule, whereby:</p>
<p><br></p>
<ul>
<li>68% between +/- 1 standard deviation from the mean</li>
<li>95% between +/- 2 standard deviations away from the mean</li>
<li>99.7% between +/- 3 standard deviations away from the mean</li>
</ul>
<p><br></p>
<p>In real life, however, you are not likely to conduct general intelligence tests on 100 probationers, 1,000 times. Instead, you are likely to administer tests to one sample, and you will have to ensure that your sample is a good one. Random sampling is one approach, as we have done so far, but another important aspect to think about is the <strong>sample size</strong>. The next activity explores this.</p>
<p><br>
<br></p>
<hr />
</div>
<div id="activity-5-sample-sizes" class="section level4">
<h4><span class="header-section-number">5.2.1.6</span> Activity 5: Sample sizes</h4>
<p>In our previous activity, we used sample sizes of 100, but is 100 a good size? What if we had repeated samples of 30 observations instead of 100? What about 1,000?</p>
<p>Let us create three sets of 1,000 samples to see how the sample size might affect our sampling distribution of the mean.</p>
<p>We make the three new data frame objects to represent these sets of 1,000 samples, and call them <code>sample30</code>, <code>sample100</code>, and <code>sample1000</code>:
<br></p>
<div class="sourceCode" id="cb284"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb284-1" title="1"><span class="co"># 30 probationers in each sample </span></a>
<a class="sourceLine" id="cb284-2" title="2">sample30 &lt;-<span class="st"> </span><span class="kw">do</span>(<span class="dv">1000</span>) <span class="op">*</span><span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> prob_off, <span class="dt">size =</span> <span class="dv">30</span>) </a>
<a class="sourceLine" id="cb284-3" title="3"></a>
<a class="sourceLine" id="cb284-4" title="4"><span class="co"># 100 probationers in each sample </span></a>
<a class="sourceLine" id="cb284-5" title="5">sample100 &lt;-<span class="st"> </span><span class="kw">do</span>(<span class="dv">1000</span>) <span class="op">*</span><span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> prob_off, <span class="dt">size =</span> <span class="dv">100</span>) </a>
<a class="sourceLine" id="cb284-6" title="6"></a>
<a class="sourceLine" id="cb284-7" title="7"><span class="co"># 1000 probationers in each sample </span></a>
<a class="sourceLine" id="cb284-8" title="8">sample1000 &lt;-<span class="st"> </span><span class="kw">do</span>(<span class="dv">1000</span>) <span class="op">*</span><span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> prob_off, <span class="dt">size =</span> <span class="dv">1000</span>) </a></code></pre></div>
<p><br></p>
<p>Now we have three data frames, all with 1,000 samples of varying sample sizes: The first has one-thousand 30-person samples, the second with one-thousand 100-person samples, and the third with one-thousand 1,000-person samples.</p>
<p>Which one of these would you trust to best represent the population? Why? Discuss in your groups if you are in the chatty rooms.</p>
<p>After, let us create some data frame objects for each set of the 1,000 samples in which we calculate the mean IQ for each one of them:
<br></p>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb285-1" title="1"><span class="co"># Calculate the mean IQ scores for each set of 1,000 samples </span></a>
<a class="sourceLine" id="cb285-2" title="2"></a>
<a class="sourceLine" id="cb285-3" title="3">sample_means30 &lt;-<span class="st"> </span>sample30 <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb285-4" title="4"><span class="st">  </span><span class="kw">group_by</span>(.index) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb285-5" title="5"><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">meanIQ =</span> <span class="kw">mean</span>(IQ))</a></code></pre></div>
<pre><code>## `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb287-1" title="1">sample_means100 &lt;-<span class="st"> </span>sample100 <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb287-2" title="2"><span class="st">  </span><span class="kw">group_by</span>(.index) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb287-3" title="3"><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">meanIQ =</span> <span class="kw">mean</span>(IQ)) </a></code></pre></div>
<pre><code>## `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb289-1" title="1">sample_means1000 &lt;-<span class="st"> </span>sample1000 <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb289-2" title="2"><span class="st">  </span><span class="kw">group_by</span>(.index) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb289-3" title="3"><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">meanIQ =</span> <span class="kw">mean</span>(IQ)) </a></code></pre></div>
<pre><code>## `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<p><br></p>
<p>We now have three more data frames, each with the means of the 1,000 samples of varying sample sizes. We will bind these together but, first, for each set, we create a new column called <code>sample_size</code>, which will tell us the sample size:
<br></p>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb291-1" title="1">sample_means30 &lt;-<span class="st"> </span>sample_means30 <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">sample_size =</span> <span class="st">&quot;30&quot;</span>)</a>
<a class="sourceLine" id="cb291-2" title="2"></a>
<a class="sourceLine" id="cb291-3" title="3">sample_means100 &lt;-<span class="st"> </span>sample_means100 <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">sample_size =</span> <span class="st">&quot;100&quot;</span>)</a>
<a class="sourceLine" id="cb291-4" title="4"></a>
<a class="sourceLine" id="cb291-5" title="5">sample_means1000 &lt;-<span class="st"> </span>sample_means1000 <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">sample_size =</span> <span class="st">&quot;1000&quot;</span>)</a></code></pre></div>
<p><br></p>
<p>Now, we use the function <code>bind_rows()</code> to <em>bind</em> these different data frames of varying sample sizes into one overall data frame in order to answer our question about a ‘good’ sample size:
<br></p>
<div class="sourceCode" id="cb292"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb292-1" title="1">sample.means.total &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(sample_means30, sample_means100, sample_means1000)</a></code></pre></div>
<p><br></p>
<p>If you view this new data frame, it has 3,000 samples in it: a thousand with 30 people in each, a thousand with 100 people in each, and a thousand with 1,000 people in each. We look at the distribution of the mean IQ for each of these three sets.</p>
<p>We can use the function <code>geom_density</code> to create a density plot. Let us fill these curves by <code>sample_size</code> and set the opacity to 0.5 (so we can see through each geom because they will overlap; to set this, do so with <code>alpha =</code>).
<br></p>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb293-1" title="1"><span class="co"># Density plot for comparison </span></a>
<a class="sourceLine" id="cb293-2" title="2"><span class="kw">ggplot</span>(<span class="dt">data =</span> sample.means.total) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb293-3" title="3"><span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x =</span> meanIQ, <span class="dt">fill =</span> sample_size), <span class="dt">alpha =</span> <span class="fl">0.5</span>) </a></code></pre></div>
<p><img src="05-inferential-statistics_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p><br></p>
<p>Density plots are useful if we want to compare distributions of a numeric variable across levels of a categorical variable. These plots enable comparisons because they standardise the frequencies of each level or group - the area under each curve of the density plots adds up to 1. If you want to compare two or three groups/levels, the density plot is the most appropriate. To read more on these plots, click <a href="https://clauswilke.com/dataviz/histograms-density-plots.html">here</a>.</p>
<p>In this present case, the numeric variable is <code>meanIQ</code> and the categorical variable is <code>sample_size</code>. From the density plot, all three sample distributions are normally distributed and have similar means to that of the population. (Remember that the mean of the sampling distribution will be the true population estimate [i.e., parameter].)</p>
<p>Notice, however, that the <em>larger the sample size, the more likely that the sample means are closer to those of the population</em>. The distribution of sample sizes of 1,000, for example, is tight and pointy, indicating that the IQ scores cluster very closely to the mean of the sampling distribution (and, therefore, the true population mean). If we look at the distribution of the sample sizes of 30, however, it is flatter and wider - its scores are more spread out from the true population mean.</p>
<p>The implication is that if we draw small sized samples, we have a higher chance of having a sample that does not reflect the true population at all. With small samples, we run the risk of getting a sample statistic that is further away from the population parameter than we would with larger samples. So, generally, the larger the sample size, the better. Otherwise, our findings and generalisations may be inaccurate.</p>
<p>What we have learned is succinctly referred to as the <strong>Central Limit Theorem</strong>. This theorem states that as sample sizes get larger, the means of the sampling distribution approaches normal distribution; it is able to reflect the true population estimate.</p>
<p>How do we know if the sample size is big enough? For this, we carry out something known as a <em>power analysis</em>, but we will get to that next week. For now, think: ‘bigger is better’ when it comes to sample size.</p>
<p>These demonstrations of sampling variability, sampling distribution, and sample sizes serve as evidence of why you can trust samples to accurately represent your population of interest. But we reiterate this once more: we often do not know the estimates of our population of interest. In addition, we may not be too sure if our one sample represents our population well. There is actually quite a lot of uncertainty in the real world of data analysis.</p>
<p>One approach to articulate this uncertainty is by quantifying our sample variabilty. We do so to get an idea of the extent to which we can trust our sample to represent the characteristics of the population - how precise is it? One quantification of variability is the <strong>standard error</strong>.</p>
<p><br>
<br></p>
<hr />
</div>
</div>
<div id="the-standard-error" class="section level3">
<h3><span class="header-section-number">5.2.2</span> The Standard Error</h3>
<p><br></p>
<div id="activity-6-interpreting-the-se" class="section level4">
<h4><span class="header-section-number">5.2.2.1</span> Activity 6: Interpreting the SE</h4>
<p>We can summarise the variability of the sampling distribution in an estimate called the standard error (SE). Whereas the standard deviation has to do with describing the data (hence, it appeared in Lesson 4 on descriptive statistics), the SE has more to do with inference (hence, its appearance in this lesson on inferential statistics). The SE is essentially the standard deviation of the sampling distribution.</p>
<p>If we are concerned with the sample mean, for example, the SE is helpful to gauge the extent to which the mean of the sample we have, drawn from a population whose estimates are unknown to us, is an accurate estimation of the true mean in that population. In other words, how different is the sample mean from the population mean.</p>
<p>We demonstrate how sample size affects the SE, in that the larger the sample size, the smaller the SE and vice versa. We specifically focus on the SE of the mean:
<br></p>
<div class="sourceCode" id="cb294"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb294-1" title="1"><span class="kw">sd</span>(sample_means30<span class="op">$</span>meanIQ)</a></code></pre></div>
<pre><code>## [1] 2.802161</code></pre>
<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb296-1" title="1"><span class="kw">sd</span>(sample_means100<span class="op">$</span>meanIQ)</a></code></pre></div>
<pre><code>## [1] 1.468766</code></pre>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb298-1" title="1"><span class="kw">sd</span>(sample_means1000<span class="op">$</span>meanIQ)</a></code></pre></div>
<pre><code>## [1] 0.474146</code></pre>
<p><br></p>
<p>Compare the values of the three sampling distributions: as the sample size gets larger, we see a smaller value for the standard error - the central limit theorem is in action.</p>
<p>With the synthetic data, we have demonstrated how samples can estimate the population, which is usually unknown to us. The above is based on repeated samples but, most likely, we will have only one sample. To calculate the SE from one sample, we create a sample of 1,000 people:
<br></p>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb300-1" title="1"><span class="kw">set.seed</span>(<span class="dv">1234</span>) <span class="co"># setting a seed so we all have the same values</span></a>
<a class="sourceLine" id="cb300-2" title="2"></a>
<a class="sourceLine" id="cb300-3" title="3">new_<span class="dv">1000</span>_sample &lt;-<span class="st"> </span><span class="kw">sample</span>(prob_off, <span class="dv">1000</span>)</a></code></pre></div>
<p><br></p>
<p>We get the SE by dividing the standard deviation (SD or <em>s</em>) of the variable <code>IQ</code> from the data frame <code>new_1000_sample</code> by the square root of our sample size (n = 1,000):</p>
<p><br></p>
<p><span class="math inline">\(SE = s/\sqrt(n)\)</span></p>
<p><br>
<br></p>
<p>In <code>R</code>:
<br></p>
<div class="sourceCode" id="cb301"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb301-1" title="1"><span class="kw">sd</span>(new_<span class="dv">1000</span>_sample<span class="op">$</span>IQ)<span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">1000</span>)</a></code></pre></div>
<pre><code>## [1] 0.4887421</code></pre>
<p><br></p>
<p>The SE (of the mean) is 0.4887421. How to interpret and communicate this estimate? A way of talking about this has to do with the <em>68-95-99.7 rule</em>.</p>
<p>With our above SE, and because we are dealing with the normal distribution, we can say that 95% of the sample will produce a mean which is within above or below 2 * 0.4887421; in other words, within +/- 0.9774842, or within approximately one IQ point above and below the mean IQ for the whole population of 3.6 million probationers – this is close to the true estimate.</p>
<p>This is how we can use the sampling distribution of the mean to estimate how representative the sample mean is to that of the population.</p>
<p>Reka exclaims: ‘How cool is that?! It is the power of statistics all in our hands!’</p>
<p><br>
<br></p>
<hr />
</div>
</div>
<div id="confidence-intervals" class="section level3">
<h3><span class="header-section-number">5.2.3</span> Confidence Intervals</h3>
<p>The last thing we will learn about is a better way of communicating the extent of inaccuracy in sample estimates. Communicating uncertainty when talking about statistics is an incredibly important topic! In inferential statistics, we are making generalisations - making inferences about a population based on some data we collected from a sample. This means that we will always have some element of error and uncertainty in our conclusions.</p>
<p>One way to clearly quantify and communicate our uncertainty is to use <strong>confidence intervals</strong> (CIs). These appear as an interval that tells you the margin of error – how far away is your (sample) statistic from the (population) parameter. We calculate them by, first, returning to our normal distribution, and its 68-95-99.7 rule.</p>
<p><br></p>
<div id="activity-7-the-68-95-99.7-rule-in-action" class="section level4">
<h4><span class="header-section-number">5.2.3.1</span> Activity 7: The 68-95-99.7 rule in action</h4>
<p>Two observations to note: first, last time, we learned about standard deviations (SD) and there was mention of 68% of verbal assaults falling within one SD; it was a reference to this 68-95-99.7 rule. Second, there is a contradiction with the numbers. If 95% of values fall within 1.96 SD, then why does this rule state that 95% of values will fall within two SD, which we have been stating throughout this lesson too? We are simply rounding up. The former (1.96) is the precise number and the latter (2) is an approximation, meant to help you memorise this rule easier than if the value was a non-integer like 1.96.</p>
<p>If 95% of values of the normal distribution fall within 1.96 SD of the mean, we are able to calculate the upper and lower bounds of this particular confidence interval using this 1.96 value (also known as the z-value) from our sample using the following formula:</p>
<p><br></p>
<p><span class="math inline">\(\bar{x} \pm 1.96*{sd}/{\sqrt{n}}\)</span></p>
<p><br></p>
<p>Let us revisit our <code>sample1</code> object. We obtain its mean of IQ:
<br></p>
<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb303-1" title="1"><span class="kw">mean</span>(sample1<span class="op">$</span>IQ, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## [1] 101.59</code></pre>
<p><br></p>
<p>Now to create the lower bound of the CI around this sample statistic, we take the mean and subtract from it the value obtained from dividing the standard deviation by the square root of the sample size, multiplied by 1.96:
<br></p>
<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb305-1" title="1"><span class="kw">mean</span>(sample1<span class="op">$</span>IQ, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>) <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span><span class="op">*</span><span class="kw">sd</span>(sample1<span class="op">$</span>IQ)<span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">100</span>)</a></code></pre></div>
<pre><code>## [1] 98.40013</code></pre>
<p><br></p>
<p>To get the upper bound, you add the same value (obtained from 1.96 * SD/ sqrt(100)) to the mean: <br></p>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb307-1" title="1"><span class="kw">mean</span>(sample1<span class="op">$</span>IQ, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>) <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span><span class="op">*</span><span class="kw">sd</span>(sample1<span class="op">$</span>IQ)<span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">100</span>)</a></code></pre></div>
<pre><code>## [1] 104.7799</code></pre>
<p><br></p>
<!-- We can visualise these add these using a density plot for the distribution of IQ scores in our sample, and the the `geom_vline` function to show the mean (red line) and the +/- standard deviation (blue line), and the upper and lower CIs (green line): -->
<!-- ```{r} -->
<!-- ggplot(data = sample1) +  -->
<!--   geom_density(mapping = aes(x = IQ)) +  -->
<!--   geom_vline(mapping = aes(xintercept = mean(IQ)), col = "red", linetype = "dashed") + -->
<!--   geom_vline(mapping = aes(xintercept = mean(IQ) +  -->
<!--                              1.96*sd(IQ)), col = "blue", linetype = "dashed") + -->
<!--   geom_vline(mapping = aes(xintercept = mean(IQ) - 1.96*sd(IQ)), col = "blue", linetype = "dashed") +  -->
<!--   geom_vline(mapping = aes(xintercept = mean(IQ) +  -->
<!--                              1.96*sd(IQ)/sqrt(100)), col = "green", linetype = "dashed") + -->
<!--   geom_vline(mapping = aes(xintercept = mean(IQ) - 1.96*sd(IQ)/sqrt(100)), col = "green", linetype = "dashed") -->
<!-- ``` -->
<!--Seeing the dashes that represent the confidence interval shows us that IQ scores will vary away from the mean of our sample, but 95% of them will fall within this interval. Similar to what we learned about repeated samples, if we took 100 resamples of our population of probationers and obtained the sample means, the true population mean will fall within the confidence interval 95% of the time. Thus, only 5% of the time will our resamples fail to obtain the true population mean.  -->
<p>We can conclude from our sample that, 95% of the time, when we calculate the CI this way, the mean IQ for all probationers will be somewhere between 98.4001302 and 104.7798698. <strong>Do NOT</strong> ever state this as: ‘There is a 95% chance that the mean is between … and …’. To say this contradicts the fact that the true population value is fixed and unknown, and it is either inside or outside of the CI with 100% certainty.</p>
<p>What if we took a different sample though? You can repeat the steps above for sample 2 and sample 3, and you will see the following conclusions:
<br></p>
<ul>
<li><strong>Sample 2</strong>: we conclude that, when we take repeated samples from the same population, 95% of the time, the mean IQ for all probationers will be somewhere between 97.8539159 and 103.7660841.</li>
<li><strong>Sample 3</strong>: we conclude that, from 95% of our samples, the mean IQ for all probationers will be somewhere between 96.3651461 and 102.4548539.</li>
</ul>
<p><br></p>
<p>With each different sample, we get a slightly different upper and lower bound of the CIs. How can we trust this? Since we know that we have 95% of observations within 1.96 SD above and below the mean of the sampling distribution, we can conclude that on the whole, the confidence intervals derived from 95% of our samples will contain the true population parameter.</p>
<p>Seems unbelievable? Let us view this:</p>
<p>First, we take our parameter, the true mean IQ for all probationers in the US: <br></p>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb309-1" title="1"><span class="co"># Create a vector containing the true population mean from prob_off</span></a>
<a class="sourceLine" id="cb309-2" title="2">true.mean &lt;-<span class="st"> </span><span class="kw">mean</span>(prob_off<span class="op">$</span>IQ)</a></code></pre></div>
<p><br></p>
<p>Then, we create another 100 samples with 100 probationers in each:
<br></p>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb310-1" title="1"><span class="kw">set.seed</span>(<span class="dv">1897</span>)</a>
<a class="sourceLine" id="cb310-2" title="2">new_sample_<span class="dv">100</span> &lt;-<span class="st"> </span><span class="kw">do</span>(<span class="dv">100</span>) <span class="op">*</span><span class="st"> </span><span class="kw">sample</span>(prob_off, <span class="dt">size =</span> <span class="dv">100</span>)</a></code></pre></div>
<p><br></p>
<p>For each sample, we calculate the mean and the lower and upper bounds of the CI (as done above):
<br></p>
<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb311-1" title="1"><span class="co"># Select the sample of 100 samples, each with 100 probationers, and place in object, ‘new.sample.ci100’</span></a>
<a class="sourceLine" id="cb311-2" title="2">new.sample.ci100 &lt;-<span class="st"> </span>new_sample_<span class="dv">100</span> <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb311-3" title="3"><span class="st">  </span><span class="kw">group_by</span>(.index) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb311-4" title="4"><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">sample_mean =</span> <span class="kw">mean</span>(IQ), </a>
<a class="sourceLine" id="cb311-5" title="5">         <span class="dt">sample_sd =</span> <span class="kw">sd</span>(IQ),</a>
<a class="sourceLine" id="cb311-6" title="6">         <span class="dt">lower_ci =</span> sample_mean<span class="fl">-1.96</span><span class="op">*</span>sample_sd<span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">100</span>), </a>
<a class="sourceLine" id="cb311-7" title="7">         <span class="dt">upper_ci =</span> sample_mean<span class="fl">+1.96</span><span class="op">*</span>sample_sd<span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">100</span>)) </a></code></pre></div>
<pre><code>## `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<div class="sourceCode" id="cb313"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb313-1" title="1"><span class="kw">View</span>(new.sample.ci100)</a></code></pre></div>
<p><br></p>
<p>If you view this new data frame <code>new.sample.ci100</code>, you will see that for each one of our 100 samples, we have columns for the sample mean, as well as the SD and the calculated upper and lower bounds of the CIs.</p>
<p>Now to see whether each one of the CIs in our new data frame contains the true population mean (<code>true.mean</code>), we use the <code>if_else()</code> function to create an additional variable that will tell us so:
<br></p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb314-1" title="1"><span class="co"># Specify code below to be: If lower CI bound &lt; true mean AND upper CI bound &gt; true mean, then &#39;capture.mean&#39; will be &#39;yes&#39; </span></a>
<a class="sourceLine" id="cb314-2" title="2"><span class="co"># If not, &#39;capture.mean&#39; will be &#39;no&#39;</span></a>
<a class="sourceLine" id="cb314-3" title="3">new.sample.ci100 &lt;-<span class="st"> </span>new.sample.ci100 <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb314-4" title="4"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">capture.mean =</span> <span class="kw">if_else</span>(<span class="dt">condition =</span> lower_ci <span class="op">&lt;</span><span class="st"> </span>true.mean <span class="op">&amp;</span><span class="st"> </span>upper_ci <span class="op">&gt;</span><span class="st"> </span>true.mean, <span class="dt">true =</span> <span class="st">&quot;yes&quot;</span>, <span class="dt">false =</span> <span class="st">&quot;no&quot;</span>)) </a></code></pre></div>
<p><br></p>
<p>We now use this to produce a tiny table to show how many CIs captured the true population mean, as indicated in the new variable, <code>capture.mean</code>:
<br></p>
<div class="sourceCode" id="cb315"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb315-1" title="1"><span class="kw">table</span>(new.sample.ci100<span class="op">$</span>capture.mean)</a></code></pre></div>
<pre><code>## 
##  no yes 
##   4  96</code></pre>
<p><br></p>
<p>According to our tiny table, about 95% of the CIs from our samples contained the true population estimate. In this example 96% contained our true population mean, and 4% did not, and it is close to what we expected. What if we were to do this with 1,000 samples of 100? What do you think the tiny table of ‘yes’ and ‘no’ would look like then? What about 10,000 samples? Discuss in your groups.</p>
<p><br>
<br></p>
<hr />
</div>
<div id="activity-8-visualising-confidence-intervals" class="section level4">
<h4><span class="header-section-number">5.2.3.2</span> Activity 8: Visualising confidence intervals</h4>
<p>Finally, we can visualise our CIs to better understand what we have just discovered:
<br></p>
<div class="sourceCode" id="cb317"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb317-1" title="1"><span class="kw">ggplot</span>(<span class="dt">data =</span> new.sample.ci100) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb317-2" title="2"><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">xintercept =</span> true.mean), <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb317-3" title="3"><span class="st">  </span><span class="kw">geom_errorbarh</span>(<span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">xmin =</span> lower_ci, <span class="dt">xmax =</span> upper_ci, <span class="dt">y =</span> .index, <span class="dt">colour =</span> capture.mean)) <span class="op">+</span><span class="st"> </span><span class="co"># Creating error bars to represent CIs and colouring in which ones captured population mean #and did not by ‘capture.mean’</span></a>
<a class="sourceLine" id="cb317-4" title="4"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">y =</span> .index, <span class="dt">x =</span> sample_mean, <span class="dt">colour =</span> capture.mean))</a></code></pre></div>
<p><img src="05-inferential-statistics_files/figure-html/unnamed-chunk-37-1.png" width="672" />
<br></p>
<p>The visual shows the result obtained in our tiny table, but here, you can see all 100 sample means, their CIs, and how 95% of the time, they capture the true population mean. In reality, we have no way of knowing whether we have captured the true population estimates in our sample, but the use of the confidence interval gives us <em>confidence</em> that we are on the right track, so reporting CIs in your results is good practice for presenting your findings.</p>
<p><br>
<br></p>
<hr />
</div>
</div>
</div>
<div id="summary-4" class="section level2">
<h2><span class="header-section-number">5.3</span> SUMMARY</h2>
<p>Today was a theoretical demonstration of why <strong>samples</strong> can be used to estimate what is happening in the <strong>population</strong>. Samples with high <strong>external validity</strong> can do so. This is the foundation of inferential statistics - the use of samples to draw conclusions about the population. We used <strong>synthetic data</strong> to show why and how. Despite <strong>sampling variability</strong>, the means of the <strong>sampling distribution</strong> demonstrate that it is able to approximate the normal distribution and, therefore, the true population estimates. This is further demonstrated by the <strong>central limit theorem</strong>, which clarifies that sample size matters in producing more accurate estimates of the population. A nifty fact we could use to interpret results from the normal distribution was the <strong>68-95-99.7 rule</strong>. We learned about communicating uncertainty in our data by presenting the <strong>standard error</strong> (of the mean) and <strong>confidence intervals</strong>. These are useful in establishing how accurate our estimates are, because in reality, rarely are the population estimates known.</p>
<p><br>
<br></p>
<p>Homework time!</p>
<p><br>
<br></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="descriptive-statistics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="hypotheses.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/05-inferential-statistics.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Modelling-Crime-Data-2021.pdf", "Modelling-Crime-Data-2021.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
