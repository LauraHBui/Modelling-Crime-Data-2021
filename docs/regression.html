<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Regression | CRIM20452 Modelling Criminological Data</title>
  <meta name="description" content="This is a companion workbook for the 2nd year undergraduate module CRIM20452 Modelling Criminological Data at the University of Manchester" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Regression | CRIM20452 Modelling Criminological Data" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a companion workbook for the 2nd year undergraduate module CRIM20452 Modelling Criminological Data at the University of Manchester" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Regression | CRIM20452 Modelling Criminological Data" />
  
  <meta name="twitter:description" content="This is a companion workbook for the 2nd year undergraduate module CRIM20452 Modelling Criminological Data at the University of Manchester" />
  

<meta name="author" content="Laura Bui &amp; Reka Solymosi" />


<meta name="date" content="2021-04-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="more-on-effect-sizes.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modelling Criminological Data</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="1" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html"><i class="fa fa-check"></i><b>1</b> A First Lesson About R</a><ul>
<li class="chapter" data-level="1.1" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#install-r-r-studio"><i class="fa fa-check"></i><b>1.1</b> Install R &amp; R Studio</a><ul>
<li class="chapter" data-level="1.1.1" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#activity-1-identifying-your-operating-system"><i class="fa fa-check"></i><b>1.1.1</b> Activity 1: Identifying your operating system</a></li>
<li class="chapter" data-level="1.1.2" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#activity-2-install-r-r-studio"><i class="fa fa-check"></i><b>1.1.2</b> Activity 2: Install R &amp; R Studio</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#getting-to-know-rstudio"><i class="fa fa-check"></i><b>1.2</b> Getting to know <code>RStudio</code></a><ul>
<li class="chapter" data-level="1.2.1" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#the-four-panes-of-r-studio"><i class="fa fa-check"></i><b>1.2.1</b> The four panes of R Studio</a></li>
<li class="chapter" data-level="1.2.2" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#customising-r-studio"><i class="fa fa-check"></i><b>1.2.2</b> Customising R Studio</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#todays-3-topics"><i class="fa fa-check"></i><b>1.3</b> Today’s 3 (TOPICS)</a><ul>
<li class="chapter" data-level="1.3.1" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#operators-and-functions"><i class="fa fa-check"></i><b>1.3.1</b> Operators and Functions</a></li>
<li class="chapter" data-level="1.3.2" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#objects"><i class="fa fa-check"></i><b>1.3.2</b> Objects</a></li>
<li class="chapter" data-level="1.3.3" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#packages-1"><i class="fa fa-check"></i><b>1.3.3</b> Packages</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#summary"><i class="fa fa-check"></i><b>1.4</b> SUMMARY</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html"><i class="fa fa-check"></i><b>2</b> Getting to know your data</a><ul>
<li class="chapter" data-level="2.1" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#the-tidyverse"><i class="fa fa-check"></i><b>2.1</b> The Tidyverse</a></li>
<li class="chapter" data-level="2.2" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#r-projects-getting-your-work-files-organised"><i class="fa fa-check"></i><b>2.2</b> R Projects – Getting Your Work Files Organised</a><ul>
<li class="chapter" data-level="2.2.1" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#activity-1-making-yourself-a-project"><i class="fa fa-check"></i><b>2.2.1</b> Activity 1: Making yourself a project</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#importing-data"><i class="fa fa-check"></i><b>2.3</b> Importing Data</a><ul>
<li class="chapter" data-level="2.3.1" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#activity-2-importing-and-viewing-data"><i class="fa fa-check"></i><b>2.3.1</b> Activity 2: Importing and Viewing Data</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#todays-3-topics-1"><i class="fa fa-check"></i><b>2.4</b> Today’s 3 (TOPICS)</a><ul>
<li class="chapter" data-level="2.4.1" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#variables"><i class="fa fa-check"></i><b>2.4.1</b> Variables</a></li>
<li class="chapter" data-level="2.4.2" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#labels"><i class="fa fa-check"></i><b>2.4.2</b> Labels</a></li>
<li class="chapter" data-level="2.4.3" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#subsetting"><i class="fa fa-check"></i><b>2.4.3</b> Subsetting</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#summary-1"><i class="fa fa-check"></i><b>2.5</b> SUMMARY</a><ul>
<li class="chapter" data-level="2.5.1" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#answers-to-activities-if-applicable"><i class="fa fa-check"></i><b>2.5.1</b> Answers to activities (if applicable)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data-visualization.html"><a href="data-visualization.html"><i class="fa fa-check"></i><b>3</b> Data Visualization</a><ul>
<li class="chapter" data-level="3.1" data-path="data-visualization.html"><a href="data-visualization.html#grammar-of-graphics"><i class="fa fa-check"></i><b>3.1</b> Grammar of Graphics</a><ul>
<li class="chapter" data-level="3.1.1" data-path="data-visualization.html"><a href="data-visualization.html#activity-1-getting-ready"><i class="fa fa-check"></i><b>3.1.1</b> Activity 1: Getting Ready</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="data-visualization.html"><a href="data-visualization.html#ggplot2"><i class="fa fa-check"></i><b>3.2</b> ggplot2</a></li>
<li class="chapter" data-level="3.3" data-path="data-visualization.html"><a href="data-visualization.html#todays-3"><i class="fa fa-check"></i><b>3.3</b> Today’s 3</a><ul>
<li class="chapter" data-level="3.3.1" data-path="data-visualization.html"><a href="data-visualization.html#layers"><i class="fa fa-check"></i><b>3.3.1</b> Layers</a></li>
<li class="chapter" data-level="3.3.2" data-path="data-visualization.html"><a href="data-visualization.html#graphs-for-categorical-data"><i class="fa fa-check"></i><b>3.3.2</b> Graphs for Categorical Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="data-visualization.html"><a href="data-visualization.html#graphs-for-numeric-data"><i class="fa fa-check"></i><b>3.3.3</b> Graphs for Numeric Data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="data-visualization.html"><a href="data-visualization.html#summary-2"><i class="fa fa-check"></i><b>3.4</b> SUMMARY</a><ul>
<li class="chapter" data-level="3.4.1" data-path="data-visualization.html"><a href="data-visualization.html#answers-to-activities"><i class="fa fa-check"></i><b>3.4.1</b> ANSWERS TO ACTIVITIES:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html"><i class="fa fa-check"></i><b>4</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="4.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#revisiting-descriptive-statistics"><i class="fa fa-check"></i><b>4.1</b> Revisiting Descriptive Statistics</a><ul>
<li class="chapter" data-level="4.1.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#activity-1-our-preparation-routine"><i class="fa fa-check"></i><b>4.1.1</b> Activity 1: Our preparation routine</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#todays-3-1"><i class="fa fa-check"></i><b>4.2</b> Today’s 3</a><ul>
<li class="chapter" data-level="4.2.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>4.2.1</b> Central Tendency</a></li>
<li class="chapter" data-level="4.2.2" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#outliers"><i class="fa fa-check"></i><b>4.2.2</b> Outliers</a></li>
<li class="chapter" data-level="4.2.3" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#measures-of-dispersion"><i class="fa fa-check"></i><b>4.2.3</b> Measures of Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#summary-3"><i class="fa fa-check"></i><b>4.3</b> SUMMARY</a><ul>
<li class="chapter" data-level="4.3.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#answers-to-activities-where-applicable"><i class="fa fa-check"></i><b>4.3.1</b> Answers to Activities (where applicable)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="inferential-statistics.html"><a href="inferential-statistics.html"><i class="fa fa-check"></i><b>5</b> Inferential Statistics</a><ul>
<li class="chapter" data-level="5.1" data-path="inferential-statistics.html"><a href="inferential-statistics.html#generalising-about-the-world-from-data"><i class="fa fa-check"></i><b>5.1</b> Generalising About the World from Data</a><ul>
<li class="chapter" data-level="5.1.1" data-path="inferential-statistics.html"><a href="inferential-statistics.html#activity-1-our-preparation-routine-1"><i class="fa fa-check"></i><b>5.1.1</b> Activity 1: Our preparation routine</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="inferential-statistics.html"><a href="inferential-statistics.html#todays-3-2"><i class="fa fa-check"></i><b>5.2</b> Today’s 3</a><ul>
<li class="chapter" data-level="5.2.1" data-path="inferential-statistics.html"><a href="inferential-statistics.html#samples"><i class="fa fa-check"></i><b>5.2.1</b> Samples</a></li>
<li class="chapter" data-level="5.2.2" data-path="inferential-statistics.html"><a href="inferential-statistics.html#the-standard-error"><i class="fa fa-check"></i><b>5.2.2</b> The Standard Error</a></li>
<li class="chapter" data-level="5.2.3" data-path="inferential-statistics.html"><a href="inferential-statistics.html#confidence-intervals"><i class="fa fa-check"></i><b>5.2.3</b> Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="inferential-statistics.html"><a href="inferential-statistics.html#summary-4"><i class="fa fa-check"></i><b>5.3</b> SUMMARY</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="hypotheses.html"><a href="hypotheses.html"><i class="fa fa-check"></i><b>6</b> Hypotheses</a><ul>
<li class="chapter" data-level="6.1" data-path="hypotheses.html"><a href="hypotheses.html#hypothesis-testing"><i class="fa fa-check"></i><b>6.1</b> Hypothesis Testing</a><ul>
<li class="chapter" data-level="6.1.1" data-path="hypotheses.html"><a href="hypotheses.html#the-null-hypothesis"><i class="fa fa-check"></i><b>6.1.1</b> The null hypothesis</a></li>
<li class="chapter" data-level="6.1.2" data-path="hypotheses.html"><a href="hypotheses.html#directional-hypotheses"><i class="fa fa-check"></i><b>6.1.2</b> Directional hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="hypotheses.html"><a href="hypotheses.html#todays-3-3"><i class="fa fa-check"></i><b>6.2</b> Today’s 3</a><ul>
<li class="chapter" data-level="6.2.1" data-path="hypotheses.html"><a href="hypotheses.html#statistical-significance"><i class="fa fa-check"></i><b>6.2.1</b> Statistical Significance</a></li>
<li class="chapter" data-level="6.2.2" data-path="hypotheses.html"><a href="hypotheses.html#hypothesis-tests-for-the-binomial-distribution"><i class="fa fa-check"></i><b>6.2.2</b> Hypothesis Tests for the Binomial Distribution</a></li>
<li class="chapter" data-level="6.2.3" data-path="hypotheses.html"><a href="hypotheses.html#hypothesis-tests-for-the-normal-distribution"><i class="fa fa-check"></i><b>6.2.3</b> Hypothesis Tests for the Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="hypotheses.html"><a href="hypotheses.html#summary-5"><i class="fa fa-check"></i><b>6.3</b> SUMMARY</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="relationships-with-categorical-variables.html"><a href="relationships-with-categorical-variables.html"><i class="fa fa-check"></i><b>7</b> Relationships with Categorical Variables</a><ul>
<li class="chapter" data-level="7.1" data-path="relationships-with-categorical-variables.html"><a href="relationships-with-categorical-variables.html#associating-with-categorical-variables"><i class="fa fa-check"></i><b>7.1</b> Associating with Categorical Variables</a><ul>
<li class="chapter" data-level="7.1.1" data-path="relationships-with-categorical-variables.html"><a href="relationships-with-categorical-variables.html#activity-1-our-r-prep-routine"><i class="fa fa-check"></i><b>7.1.1</b> Activity 1: Our <code>R</code> prep routine</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="relationships-with-categorical-variables.html"><a href="relationships-with-categorical-variables.html#todays-3-4"><i class="fa fa-check"></i><b>7.2</b> Today’s 3</a><ul>
<li class="chapter" data-level="7.2.1" data-path="relationships-with-categorical-variables.html"><a href="relationships-with-categorical-variables.html#independent-and-dependent-variables"><i class="fa fa-check"></i><b>7.2.1</b> Independent and Dependent Variables</a></li>
<li class="chapter" data-level="7.2.2" data-path="relationships-with-categorical-variables.html"><a href="relationships-with-categorical-variables.html#comparing-means-the-t-test"><i class="fa fa-check"></i><b>7.2.2</b> Comparing means: the t-test</a></li>
<li class="chapter" data-level="7.2.3" data-path="relationships-with-categorical-variables.html"><a href="relationships-with-categorical-variables.html#chi-square"><i class="fa fa-check"></i><b>7.2.3</b> Chi-square</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="relationships-with-categorical-variables.html"><a href="relationships-with-categorical-variables.html#summary-6"><i class="fa fa-check"></i><b>7.3</b> SUMMARY</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="more-on-effect-sizes.html"><a href="more-on-effect-sizes.html"><i class="fa fa-check"></i><b>8</b> (More) On Effect Sizes</a><ul>
<li class="chapter" data-level="8.1" data-path="more-on-effect-sizes.html"><a href="more-on-effect-sizes.html#measures-of-association-a-continuation"><i class="fa fa-check"></i><b>8.1</b> Measures of Association: A Continuation</a><ul>
<li class="chapter" data-level="8.1.1" data-path="more-on-effect-sizes.html"><a href="more-on-effect-sizes.html#activity-1-the-r-prep-routine"><i class="fa fa-check"></i><b>8.1.1</b> Activity 1: The <code>R</code> prep routine</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="more-on-effect-sizes.html"><a href="more-on-effect-sizes.html#todays-2"><i class="fa fa-check"></i><b>8.2</b> Today’s 2 (!)</a><ul>
<li class="chapter" data-level="8.2.1" data-path="more-on-effect-sizes.html"><a href="more-on-effect-sizes.html#correlations"><i class="fa fa-check"></i><b>8.2.1</b> Correlations</a></li>
<li class="chapter" data-level="8.2.2" data-path="more-on-effect-sizes.html"><a href="more-on-effect-sizes.html#power-analysis"><i class="fa fa-check"></i><b>8.2.2</b> Power analysis</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="more-on-effect-sizes.html"><a href="more-on-effect-sizes.html#summary-7"><i class="fa fa-check"></i><b>8.3</b> SUMMARY</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>9</b> Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="regression.html"><a href="regression.html#multiple-and-simultaneous-relationships"><i class="fa fa-check"></i><b>9.1</b> Multiple and Simultaneous Relationships</a></li>
<li class="chapter" data-level="9.2" data-path="regression.html"><a href="regression.html#todays-3-5"><i class="fa fa-check"></i><b>9.2</b> Today’s 3</a><ul>
<li class="chapter" data-level="9.2.1" data-path="regression.html"><a href="regression.html#motivating-regression"><i class="fa fa-check"></i><b>9.2.1</b> Motivating Regression</a></li>
<li class="chapter" data-level="9.2.2" data-path="regression.html"><a href="regression.html#interpreting-ols-regression"><i class="fa fa-check"></i><b>9.2.2</b> Interpreting OLS Regression</a></li>
<li class="chapter" data-level="9.2.3" data-path="regression.html"><a href="regression.html#logistic-regression"><i class="fa fa-check"></i><b>9.2.3</b> Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="regression.html"><a href="regression.html#summary-8"><i class="fa fa-check"></i><b>9.3</b> SUMMARY</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">CRIM20452 Modelling Criminological Data</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Regression</h1>
<div id="ols-logistic-regressions" class="section level4 unnumbered">
<h4><em>OLS &amp; Logistic Regressions</em></h4>
</div>
<div id="learning-outcomes-8" class="section level4 unnumbered">
<h4>Learning Outcomes:</h4>
<ul>
<li>Learn how and when to use ordinary least squares and logistic regressions</li>
<li>Understand and interpret the outputs generated from running regression</li>
<li>Test for model fit and whether assumptions are met</li>
</ul>
</div>
<div id="todays-learning-tools-8" class="section level4 unnumbered">
<h4>Today’s Learning Tools:</h4>
<div id="total-number-of-activities-12-2" class="section level5 unnumbered">
<h5><em>Total number of activities</em>: 12</h5>
</div>
<div id="data-8" class="section level5 unnumbered">
<h5><em>Data:</em></h5>
<ul>
<li>Crime Survey for England and Wales (CSEW) from 2013 to 2014 sweep teaching dataset</li>
<li><code>Arrests</code> from the <code>effects</code> package</li>
</ul>
</div>
<div id="packages-9" class="section level5 unnumbered">
<h5><em>Packages:</em></h5>
<ul>
<li><code>arm</code></li>
<li><code>car</code></li>
<li><code>effects</code></li>
<li><code>ggplot2</code></li>
<li><code>here</code></li>
<li><code>lessR</code></li>
<li><code>sjPlot</code></li>
<li><code>tidyverse</code></li>
</ul>
</div>
<div id="functions-introduced-and-packages-to-which-they-belong-8" class="section level5 unnumbered">
<h5><em>Functions introduced (and packages to which they belong)</em></h5>
<ul>
<li><code>complete.cases()</code> : Returns only complete cases that do not have NAs (<code>base R</code>)</li>
<li><code>display()</code> : Gives a clean printout of lm, glm, and other such objects (<code>arm</code>)</li>
<li><code>lm()</code> : Fit linear models (<code>base R</code>)</li>
<li><code>Logit()</code> : Fit logistic regression models with less typing (<code>lessR</code>)</li>
<li><code>plot_model()</code> : Plots odds ratios and their confidence intervals (<code>sjPlot</code>)</li>
<li><code>relevel()</code> : Reorders the levels of a factor (<code>base R</code>)</li>
<li><code>tab_model()</code> : Creates HTML tables summarising regression models (<code>sjPlot</code>)</li>
<li><code>vif()</code> : Calculate the variance inflation for OLS or other linear models (<code>car</code>)</li>
</ul>
<p><br>
<br></p>
<hr />
</div>
</div>
<div id="multiple-and-simultaneous-relationships" class="section level2">
<h2><span class="header-section-number">9.1</span> Multiple and Simultaneous Relationships</h2>
<p>Our learning on inferential statistics, so far, has been on single relationships between two variables. One of the major drawbacks of previous analyses of single relationships is that, even though you are able to test for statistical significance, you cannot ascertain prediction very well. In other words, you were unable to say for certain that one variable predicted another variable; we merely could say ‘there was a relationship’ or ‘there was no relationship’.</p>
<p>Although we got into the habit of arranging our variables as independent and dependent variables, these terms are more relevant to regression. The reason is, not only can we establish whether relationships are statistically significant, we also can say with relatively more clarity, how that relationship looks like – how change in one variable may be impacting change in the other variable.</p>
<p>Another major drawback of previous analyses of bivariate relationships is that you are less certain about whether the relationship and its effect size would still hold in the face of other variables. For example, would a significant relationship between peer group and violence still exist when the relationship between unemployment and violence is considered? How about the relationship between previous criminal justice contacts and violence? This last lesson is on analyses that establish predictive relationships and test multiple relationships between two variables at the same time.</p>
<hr />
</div>
<div id="todays-3-5" class="section level2">
<h2><span class="header-section-number">9.2</span> Today’s 3</h2>
<p>Our main topic of the day is <strong>regression</strong> and we learn two forms: ordinary least squares (OLS) and logistic. The former is applied to dependent variables that are measured at interval or ratio level, while the latter is applied to dependent variables that are binary and measured at the nominal level. The three topics are: motivating regression; interpreting OLS regression; and logistic regression.</p>
<hr />
<div id="motivating-regression" class="section level3">
<h3><span class="header-section-number">9.2.1</span> Motivating Regression</h3>
<p>In science, one of our main concerns is to develop <strong>models</strong> of the world – models that help us to better understand the world or to predict how things will develop. You can read more about modelling in scientific research <a href="https://www.visionlearning.com/en/library/Process-of-Science/49/Modeling-in-Scientific-Research/153">here</a>. Statistics provides a set of tools that help researchers build and test scientific models.</p>
<p>Our models can be simple. We may think that unemployment is a variable that can help us to understand why cities differ in their level of violent crime. We could express such a model like Figure 9.1:</p>
<p><br></p>
<div class="figure">
<img src="Images/model1.png" alt="Figure 9.1 Simple Model" />
<p class="caption"><strong>Figure 9.1</strong> Simple Model</p>
</div>
<p><br></p>
<p>But we know the world is complex, and that likely, there are other things that may help us to understand why some cities have more violent crime than others. So, we want to have tools that allow us to examine such models that include other variables. For example, the one below, Figure 9.2:</p>
<p><br></p>
<div class="figure">
<img src="Images/model2.png" alt="Figure 9.2 A relatively complex model" />
<p class="caption"><strong>Figure 9.2</strong> A relatively complex model</p>
</div>
<p><br></p>
<p>In this lesson, we are going to cover regression analysis or, rather, we are will begin to talk about it.</p>
<p>Specifically, we start with a popular technique called <strong>ordinary least squares (OLS)</strong> regression. It is used to explore whether one or multiple independent variables (IV or <span class="math inline">\(X\)</span>) can predict or explain the variation in the dependent variable (DV or <span class="math inline">\(Y\)</span>).</p>
<p>When multiple IVs are included simultaneously, they are called <strong>covariates</strong>; when only one IV is used, the OLS regression is called <strong>bivariate regression</strong> or <em>simple regression</em>. OLS regression has been the mainstay analysis in the social sciences.</p>
<p><br></p>
<div id="activity-1-setting-up-the-r-prep-routine" class="section level4">
<h4><span class="header-section-number">9.2.1.1</span> Activity 1: Setting up! The <code>R</code> prep routine</h4>
<p>To begin, we do the following:</p>
<ol style="list-style-type: decimal">
<li><p>Open up your existing <code>R</code> project</p></li>
<li><p>Install and load the required packages shown above</p></li>
<li><p>Load the dataset ‘csew1314_teaching.csv’, using the <code>read_csv()</code> function</p></li>
<li><p>Name the data frame <code>df</code></p></li>
</ol>
<p><br>
<br></p>
<hr />
</div>
<div id="activity-2-getting-to-know-your-data" class="section level4">
<h4><span class="header-section-number">9.2.1.2</span> Activity 2: Getting to know your data</h4>
<p>The first step in any analysis is to get to know your data. We have been here before: read the codebook/ data dictionary; run summary statistics for your numeric variables, frequency distributions for your categorical variables (see Lesson 4); and visualise your variables (see Lesson 3). This will help you detect any anomalies and give you a sense for what you have.</p>
<p>Some variables of interest that we could explore are perception of antisocial behaviour (<code>antisocx</code>) and whether it is associated with other variables such as age (<code>age</code>), gender (<code>gender</code>), victimisation experience (<code>bcsvictim</code>), and confidence in the criminal justice system (<code>confx</code>).</p>
<!--You should explore these data, using appropriate methods informed by their levels of measurement. Thus, you might look at measures of central tendency and dispersion for the numeric variables or frequency tables for your categorical variables (see Lesson 4).-->
<p>For example, if we start with perception of antisocial behaviour (<code>antisocx</code>) and age (<code>age</code>), we would visualise our variable distributions:
<br></p>
<div class="sourceCode" id="cb511"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb511-1" title="1"><span class="co"># Histogram for age</span></a>
<a class="sourceLine" id="cb511-2" title="2"><span class="kw">ggplot</span>(<span class="dt">data =</span> df, <span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x =</span> age)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb511-3" title="3"><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> <span class="fl">.5</span>)</a></code></pre></div>
<p><img src="09-regression_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<div class="sourceCode" id="cb512"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb512-1" title="1"><span class="co"># Histogram for &#39;antisocx&#39;</span></a>
<a class="sourceLine" id="cb512-2" title="2"><span class="kw">ggplot</span>(<span class="dt">data =</span> df, <span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x =</span> antisocx)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb512-3" title="3"><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> <span class="fl">.2</span>) <span class="co"># antisocx is negatively skewed</span></a></code></pre></div>
<p><img src="09-regression_files/figure-html/unnamed-chunk-3-2.png" width="672" /></p>
<p><br></p>
<p><br></p>
<hr />
</div>
<div id="activity-3-exploring-your-data-further-deleting-missing-data" class="section level4">
<h4><span class="header-section-number">9.2.1.3</span> Activity 3: Exploring your data further: deleting missing data</h4>
<p>We need to look for any missing data as it may affect getting to know our data:
<br></p>
<div class="sourceCode" id="cb513"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb513-1" title="1"><span class="co"># Age variable</span></a>
<a class="sourceLine" id="cb513-2" title="2"><span class="kw">summary</span>(df<span class="op">$</span>age) <span class="co"># NA = 188</span></a></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   16.00   36.00   51.00   51.19   66.00   99.00     118</code></pre>
<div class="sourceCode" id="cb515"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb515-1" title="1"><span class="co"># Antisocial variable</span></a>
<a class="sourceLine" id="cb515-2" title="2"><span class="kw">summary</span>(df<span class="op">$</span>antisocx) <span class="co"># NA = 26695</span></a></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##  -4.015  -0.559   0.177   0.000   0.788   1.215   26695</code></pre>
<p><br></p>
<p>Both variables have missing data. You could ignore the ‘NAs’, as when we run regression, it will conduct a <em>listwise deletion</em>, where observations are dropped from your analysis if they have missing data on either the IV (<span class="math inline">\(X\)</span>) or the DV (<span class="math inline">\(Y\)</span>). But as we want descriptive statistics of only observations we will be using, we will delete those that will be dropped anyway.</p>
<p>We use the <code>complete.cases()</code> function to keep cases that have valid responses in both the IV and DV:
<br></p>
<div class="sourceCode" id="cb517"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb517-1" title="1"><span class="co"># Start with 35,371 cases</span></a>
<a class="sourceLine" id="cb517-2" title="2"><span class="kw">nrow</span>(df) </a></code></pre></div>
<pre><code>## [1] 35371</code></pre>
<div class="sourceCode" id="cb519"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb519-1" title="1"><span class="co"># Retain only complete cases</span></a>
<a class="sourceLine" id="cb519-2" title="2"></a>
<a class="sourceLine" id="cb519-3" title="3">df &lt;-<span class="st"> </span>df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(<span class="kw">complete.cases</span>(df<span class="op">$</span>age) <span class="op">&amp;</span><span class="st"> </span><span class="kw">complete.cases</span>(df<span class="op">$</span>antisocx))</a>
<a class="sourceLine" id="cb519-4" title="4"></a>
<a class="sourceLine" id="cb519-5" title="5"><span class="co"># Left with 8,650 cases after dropping NAs</span></a>
<a class="sourceLine" id="cb519-6" title="6"><span class="kw">nrow</span>(df) </a></code></pre></div>
<pre><code>## [1] 8650</code></pre>
<p><br></p>
<p>Now we can get to know the data!
<br></p>
<div class="sourceCode" id="cb521"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb521-1" title="1"><span class="co"># Summaries of just the 8,650 cases </span></a>
<a class="sourceLine" id="cb521-2" title="2"><span class="kw">summary</span>(df<span class="op">$</span>age)</a></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   16.00   36.00   50.00   50.82   66.00   98.00</code></pre>
<div class="sourceCode" id="cb523"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb523-1" title="1"><span class="kw">summary</span>(df<span class="op">$</span>antisocx)</a></code></pre></div>
<pre><code>##      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
## -4.014557 -0.552077  0.178767  0.001823  0.788219  1.215267</code></pre>
<p><br>
<br></p>
<hr />
</div>
<div id="activity-4-what-questions-can-regression-answer" class="section level4">
<h4><span class="header-section-number">9.2.1.4</span> Activity 4: What questions can regression answer?</h4>
<p>For this example, we are interested in the bivariate relationship between age (<span class="math inline">\(X\)</span>) and perception of antisocial behaviour (<span class="math inline">\(Y\)</span>).</p>
<p>Our research question is: ‘Does age <em>predict</em> perceived level of antisocial behaviour in one’s neighbourhood?’ Our null and alternative hypotheses are as follows:</p>
<p><br></p>
<p><span class="math inline">\(H_0\)</span>: Age does not predict perceived level of antisocial behaviour in one’s neighbourhood.</p>
<p><span class="math inline">\(H_A\)</span>: Age does predict perceived level of antisocial behaviour in one’s neighbourhood.</p>
<p><br></p>
<p>Returning to the concept of models as to answer ‘what questions can regression answer?’, a model can be anything that allows for inferences about a population (from a sample) to be drawn. For example, earlier in the semester, we used the mean, which could be used to draw inferences. Here, we are interested developing a model to predict perception of antisocial behaviour (<code>antisocx</code>).</p>
<p>If Reka picked a person at random from the sample, what would be your best guess on that random person’s perceived level of neighbourhood antisocial behaviour? Likely, you would say a value near the mean (M = 0.001823) of the sample. This is a good guess, but we do not know how accurate this is.</p>
<p>A model helps our guesses become more accurate. Once we model the relationship between <code>antisocx</code>, this dependent variable, and an independent variable, we can use the model to help improve our guesses. Let us start by looking at a scatterplot:
<br></p>
<div class="sourceCode" id="cb525"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb525-1" title="1"><span class="co"># Checking out how the variables covary together </span></a>
<a class="sourceLine" id="cb525-2" title="2"><span class="co"># (See Lesson 8, section 8.2.1.1 on covariation)</span></a>
<a class="sourceLine" id="cb525-3" title="3"></a>
<a class="sourceLine" id="cb525-4" title="4"><span class="kw">ggplot</span>(df, <span class="kw">aes</span>(<span class="dt">x =</span> age, <span class="dt">y =</span> antisocx)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb525-5" title="5"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha=</span>.<span class="dv">2</span>, <span class="dt">position=</span><span class="st">&quot;jitter&quot;</span>)</a></code></pre></div>
<p><img src="09-regression_files/figure-html/unnamed-chunk-7-1.png" width="672" />
<br></p>
<p>The scatterplot shows how the two variables vary together (covary). Again, you are to guess that random person’s perceived level of antisocial behaviour in the neighbourhood, but, this time, you learn random-person-from-the-sample is 30 years old. What would your answer be now?</p>
<p>Before, we based our guess on descriptive measures of central tendency and dispersion, which included all ages in the sample. Now, we are asked to guess knowing that Random Person is aged 30. This time, we would try and guess the mean of all those who also are aged 30. This is what is known as the <em>conditional mean</em> – the mean of <span class="math inline">\(Y\)</span> for each value of <span class="math inline">\(X\)</span>.</p>
<p>We now draw a trend line through the scatterplot, which represents the mean of the variable <code>antisocx</code> (on the y-axis) for each age:
<br></p>
<div class="sourceCode" id="cb526"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb526-1" title="1"><span class="kw">ggplot</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb526-2" title="2"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data=</span>df, <span class="kw">aes</span>(<span class="dt">x =</span> age, <span class="dt">y =</span> antisocx), <span class="dt">alpha=</span>.<span class="dv">2</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb526-3" title="3"><span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data=</span>df, <span class="kw">aes</span>(<span class="dt">x =</span> age, <span class="dt">y =</span> antisocx), <span class="dt">stat=</span><span class="st">&#39;summary&#39;</span>, <span class="dt">fun.y=</span>mean,</a>
<a class="sourceLine" id="cb526-4" title="4">            <span class="dt">color=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">size=</span><span class="dv">1</span>)  </a></code></pre></div>
<pre><code>## No summary function supplied, defaulting to `mean_se()`</code></pre>
<p><img src="09-regression_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p><br></p>
<p>Plotting the conditional means as represented by the blue trend line shows us that the mean of perceived antisocial behaviour for those persons aged 30 is around −0.3. This would be a better guess than the mean for all ages, 0.001823. The trend line gives us a better idea of what is going on in our scatterplot, but the line looks a bit rough. We can make it smoother:
<br></p>
<div class="sourceCode" id="cb528"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb528-1" title="1"><span class="co"># Make smoother by calculating average perceived antisocial behaviour for age in increments of 5 years</span></a>
<a class="sourceLine" id="cb528-2" title="2"></a>
<a class="sourceLine" id="cb528-3" title="3"><span class="kw">ggplot</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb528-4" title="4"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data=</span>df, <span class="kw">aes</span>(<span class="dt">x =</span> age, <span class="dt">y =</span> antisocx), <span class="dt">alpha=</span>.<span class="dv">2</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb528-5" title="5"><span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data=</span>df, <span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">round</span>(age<span class="op">/</span><span class="dv">5</span>)<span class="op">*</span><span class="dv">5</span>, <span class="dt">y =</span> antisocx), <span class="dt">stat=</span><span class="st">&#39;summary&#39;</span>, <span class="dt">fun.y=</span>mean,<span class="dt">color=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">size=</span><span class="dv">1</span>)</a></code></pre></div>
<pre><code>## No summary function supplied, defaulting to `mean_se()`</code></pre>
<p><img src="09-regression_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p><br></p>
<p>The blue trend line shows us that there is an upward trend, meaning that as age increases, so do levels of perceived neighbourhood antisocial behaviour. Likewise, OLS regression tries to capture the trend or pattern of the data by drawing a straight line of predicted values; this is considered the <em>model</em> of the data:
<br></p>
<div class="sourceCode" id="cb530"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb530-1" title="1"><span class="co"># method=lm asks for the linear regression line </span></a>
<a class="sourceLine" id="cb530-2" title="2"><span class="co"># se=FALSE asks not to print confidence interval </span></a>
<a class="sourceLine" id="cb530-3" title="3"><span class="co"># Other arguments specify the colour and thickness of the line </span></a>
<a class="sourceLine" id="cb530-4" title="4"></a>
<a class="sourceLine" id="cb530-5" title="5"><span class="kw">ggplot</span>(<span class="dt">data =</span> df, <span class="kw">aes</span>(<span class="dt">x =</span> age, <span class="dt">y =</span> antisocx)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb530-6" title="6"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">.2</span>, <span class="dt">position =</span> <span class="st">&quot;jitter&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb530-7" title="7"><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">size =</span> <span class="dv">1</span>)</a></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="09-regression_files/figure-html/unnamed-chunk-10-1.png" width="672" />
<br></p>
<p>The red <strong>regression line</strong> produces guesses or predictions for the value of perceived level of antisocial behaviour based on information that we have about age.</p>
<p>The linear line can also be seen as one that tries to summarise the pattern of what is going on among the data points. It does not go through all the points, however, and that means it is not of perfect prediction.</p>
<p>But as Bock et al. (2012) highlight:</p>
<p><br></p>
<blockquote>
<p>‘Like all models of the real world, the line will be wrong – wrong in the sense that it can’t match reality exactly. But it can help us understand how the variables are associated’ (p. 179).</p>
</blockquote>
<p><br></p>
<p>A map is never a perfect representation of the world; the same happens with statistical models. Yet, as with maps, models can be helpful, guiding us to our destination.</p>
<p>Thus, regression can answer questions relating to predictions and does so by drawing a line that tries and capture the overall trend.</p>
<p><br>
<br></p>
<hr />
</div>
<div id="activity-5-regression-assumptions" class="section level4">
<h4><span class="header-section-number">9.2.1.5</span> Activity 5: Regression assumptions</h4>
<p>To conduct OLS regression, or linear regression for short, a number of assumptions must be met, so you will need to test for them. Failure to do so will likely result in an incorrect model and drawing conclusions from it would be silly. We only, however, briefly go through the first four assumptions because going into detail is beyond the scope of this class.</p>
<p>The fifth assumption, the assumption of linearity, is an exception. Going into detail about it will give you a better understanding of OLS regression and how it operates.</p>
<p>We now go through each of the five assumptions of OLS regression before conducting it.</p>
<div id="assumption-1-independence-of-errors" class="section level5 unnumbered">
<h5>Assumption 1: Independence of errors</h5>
<p>Errors of the prediction, which make up the regression line, are assumed to be independent of each other. The term <em>heteroscedasticity</em> is used to describe this violation of independence of errors. When we violate this assumption, our points in the scatterplot resemble the shape of a funnel. If there is dependency between the observations, you would need to use other models that are not covered in this class.</p>
<p><br></p>
</div>
<div id="assumption-2-equal-variances-of-errors" class="section level5 unnumbered">
<h5>Assumption 2: Equal Variances of errors</h5>
<p>If the variance of our residuals (and what they are is explained later) is unequal, we will need different estimation methods, but this issue is minor. The reason it is a minor issue is that regression is considered a <em>robust</em> estimation, meaning that it is not too sensitive to changes in the variance.</p>
<p><br></p>
</div>
<div id="assumption-3-normality-of-errors" class="section level5 unnumbered">
<h5>Assumption 3: Normality of errors</h5>
<p>Residuals are assumed to be normally distributed. Gelman and Hill (2007) believe this to be the least important of the assumptions because regression inferences tend to be robust regarding non-normality of the errors. Your results, however, may be sensitive to large outliers, so you will need to drop them if appropriate.</p>
<p><br></p>
</div>
<div id="assumption-4-multicollinearity" class="section level5 unnumbered">
<h5>Assumption 4: Multicollinearity</h5>
<p>When you are including more than one IV, <strong>multicollinearity</strong> may be a concern. Independent variables that are highly correlated with each other mean that they are tapping into the same construct. For example, if you include two IVs, parental attachment and parental discipline, you may find that they are highly related to each other because both measure a similar idea - parenting practices perhaps.</p>
<p>One indication of multicollinearity is that if you run a Pearson’s correlation (see Lesson 8, section 8.2.1.2) for the two IVs and get a value above 0.6, you should start investigating further.</p>
<p>Another way to check for multicollinearity is through the <em>variance inflation factor</em> (VIF). How high the VIF should be before multicollinearity is a concern is debatable: here, anything above 5 should be a matter of concern. With this analysis, IVs should be numeric. We include an additional covariate, confidence in the police (<code>confx</code>), to illustrate. The VIF for our variables is conducted with the <code>vif ()</code> function in the <code>car</code> package:</p>
<p><br></p>
<div class="sourceCode" id="cb532"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb532-1" title="1"><span class="co"># Inputting our variables of interest in a model and placing in object called &#39;cal_vif&#39;</span></a>
<a class="sourceLine" id="cb532-2" title="2">cal_vif &lt;-<span class="st"> </span><span class="kw">lm</span>(antisocx <span class="op">~</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>confx, <span class="dt">data =</span> df) </a>
<a class="sourceLine" id="cb532-3" title="3"></a>
<a class="sourceLine" id="cb532-4" title="4"><span class="co"># Running VIF analysis</span></a>
<a class="sourceLine" id="cb532-5" title="5"><span class="kw">vif</span>(cal_vif)</a></code></pre></div>
<pre><code>##      age    confx 
## 1.003988 1.003988</code></pre>
<p><br></p>
<p>The VIF values are below 5 and indicate that including both variables, <code>age</code> and <code>confx</code>, in the same model does not pose any multicollinearity concerns.</p>
<p><br></p>
</div>
<div id="assumption-5-linearity" class="section level5 unnumbered">
<h5>Assumption 5: Linearity</h5>
<p>Similar to Pearson’s correlation, a linear relationship must be established before conducting the analysis. If the relationship is non-linear, your predicted values will be wrong, and it will systematically miss the true pattern of the mean of <span class="math inline">\(Y\)</span>. We will learn how to check for this when looking at the residuals from the regression.</p>
<p><br>
<br></p>
<hr />
</div>
</div>
</div>
<div id="interpreting-ols-regression" class="section level3">
<h3><span class="header-section-number">9.2.2</span> Interpreting OLS Regression</h3>
<p>In this section, we learn how to interpret bivariate and multiple regression output. Now that we know what the regression line is, how do we draw one? Two points are needed to do so:</p>
<p><br></p>
<ol style="list-style-type: decimal">
<li><p>We need to know where the line begins. The <strong>intercept</strong> is where it begins, which is the value of <span class="math inline">\(Y\)</span>, our DV, when <span class="math inline">\(X\)</span> (our IV) is 0.</p></li>
<li><p>We also need to know the angle of that line. This is referred to as the <strong>slope</strong>.</p></li>
</ol>
<p><br></p>
<p>This translates into the equation:</p>
<p><br></p>
<p><span class="math display">\[y = b_0 + b_1x_i\]</span></p>
<p><br></p>
<p>Where <span class="math inline">\(b_0\)</span> is the y-intercept and <span class="math inline">\(b_1x_i\)</span> is the slope of the line. Linear regression models try to find a line that best fits the data points and has the least error. It does so by minimising the distance from every point in the scatterplot to the regression line. This is called <em>least squares estimation</em> and is its hallmark. The farther these points are from the regression line, the more error your regression model will have.</p>
<p>It is not unusual to observe that some points will fall above the line (a positive error value), while other points will fall below it (a negative error value). If we wanted to sum these error values, the problem is that the positive values would cancel out the negative values, underreporting our overall error. This would then incorrectly suggest that our regression line was perfect.</p>
<p>To resolve this issue, the error values are squared before they are summed. This is called the <em>error sum of squares</em>. Our regression model is actually trying to find a line of best fit that has the least (squared) error. Hence, least squared estimation.</p>
<hr />
<div id="activity-6-bivariate-regression" class="section level4">
<h4><span class="header-section-number">9.2.2.1</span> Activity 6: Bivariate Regression</h4>
<p>To fit the regression model, we use the <code>lm()</code> function using the formula specification <code>(Y ~ X)</code>:
<br></p>
<div class="sourceCode" id="cb534"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb534-1" title="1"><span class="co"># Placing model into an object called &#39;fit_1&#39;</span></a>
<a class="sourceLine" id="cb534-2" title="2">fit_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">lm</span>(antisocx <span class="op">~</span><span class="st"> </span>age, <span class="dt">data =</span> df)</a>
<a class="sourceLine" id="cb534-3" title="3"></a>
<a class="sourceLine" id="cb534-4" title="4"><span class="co"># Get to know the model object</span></a>
<a class="sourceLine" id="cb534-5" title="5"><span class="kw">class</span>(fit_<span class="dv">1</span>)</a></code></pre></div>
<pre><code>## [1] &quot;lm&quot;</code></pre>
<div class="sourceCode" id="cb536"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb536-1" title="1"><span class="kw">attributes</span>(fit_<span class="dv">1</span>)</a></code></pre></div>
<pre><code>## $names
##  [1] &quot;coefficients&quot;  &quot;residuals&quot;     &quot;effects&quot;       &quot;rank&quot;         
##  [5] &quot;fitted.values&quot; &quot;assign&quot;        &quot;qr&quot;            &quot;df.residual&quot;  
##  [9] &quot;xlevels&quot;       &quot;call&quot;          &quot;terms&quot;         &quot;model&quot;        
## 
## $class
## [1] &quot;lm&quot;</code></pre>
<div class="sourceCode" id="cb538"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb538-1" title="1"><span class="kw">summary</span>(fit_<span class="dv">1</span>)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = antisocx ~ age, data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.8481 -0.5752  0.1297  0.7691  1.6581 
## 
## Coefficients:
##              Estimate Std. Error t value            Pr(&gt;|t|)
## (Intercept) -0.647232   0.030455  -21.25 &lt;0.0000000000000002
## age          0.012773   0.000563   22.68 &lt;0.0000000000000002
## 
## Residual standard error: 0.9705 on 8648 degrees of freedom
## Multiple R-squared:  0.05616,    Adjusted R-squared:  0.05606 
## F-statistic: 514.6 on 1 and 8648 DF,  p-value: &lt; 0.00000000000000022</code></pre>
<div class="sourceCode" id="cb540"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb540-1" title="1"><span class="co"># A more concise display of results is using ‘display()’ from arm package:</span></a>
<a class="sourceLine" id="cb540-2" title="2"><span class="kw">display</span>(fit_<span class="dv">1</span>)</a></code></pre></div>
<pre><code>## lm(formula = antisocx ~ age, data = df)
##             coef.est coef.se
## (Intercept) -0.65     0.03  
## age          0.01     0.00  
## ---
## n = 8650, k = 2
## residual sd = 0.97, R-Squared = 0.06</code></pre>
<p><br></p>
<p>We then interpret the regression output. There are several points to focus on when communicating your results:</p>
<p><br></p>
<ol style="list-style-type: decimal">
<li><p><strong>Beta (or regression) coefficient</strong>: this is the value that measures the impact of <span class="math inline">\(X\)</span> (IV) on <span class="math inline">\(Y\)</span> (DV). It is the <span class="math inline">\(b_1\)</span> regression coefficient from the previous equation, the value that will shape the slope for this model. From our output, this value is 0.012773. When the value is positive, it tells us that for every 1 unit increase in <span class="math inline">\(X\)</span>, there is a <span class="math inline">\(b_1\)</span> increase on <span class="math inline">\(Y\)</span>; if, however, the coefficient is negative, then it represents a decrease on <span class="math inline">\(Y\)</span>. Here, we interpret the result as: ‘For every 1 year older, there is a 0.01 unit increase in level of perceived antisocial behaviour in the neighbourhood.’ The coefficient can be interpreted as a measure of the effect size of this relationship. In addition, with very large sample sizes, you should place less of an emphasis on p-values and more emphasis on the size of the beta coefficients because p-values are sensitive to sample size. For example, you may have all statistically significant covariates, even though the variables have small coefficients. This means that they do not have much of an effect on the DV.</p></li>
<li><p><strong>P-value</strong>: The p-value for each beta coefficient tests the null hypothesis that the coefficient is equal to zero. A low p-value (<span class="math inline">\(\alpha\)</span> &lt; 0.05) means you can reject the null hypothesis and that changes in the predictor’s (<span class="math inline">\(X\)</span>) value are significantly related to changes in the <span class="math inline">\(Y\)</span> value. For statistically non-significant coefficients, it is not recommended that you interpret them; you can, however, make statements about their general trend (positive or negative). In our output, the p-value is very small ( p &lt; .001). We conclude that age significantly predicts level of perceived antisocial behaviour and that we can reject the null hypothesis.</p></li>
<li><p><strong>F-statistic</strong>: There is another p-value at the bottom of your regression output and belongs to the F-statistic. This assesses whether our overall model can predict <span class="math inline">\(Y\)</span> with high confidence, especially if we have multiple predictors. This, too, is statistically significant, meaning that at least one of our IVs must be related to our DV.</p></li>
<li><p><strong>(Multiple) <span class="math inline">\(R^2\)</span></strong>: This is known as the percent of variance explained, and it is a measure of the strength of our model. This value ranges from 0 to 1. Towards 1, the better we are able to account for variation in our outcome <span class="math inline">\(Y\)</span> with our IV(s). In other words, the stronger the relationship is between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>. Weisburd and Britt (2009: 437) suggest that, in criminal justice research, values greater than .40 are rare, but if we obtain such a value, it is considered a powerful model. If the value is lower than .20, the model is considered relatively weak. The output shows that <span class="math inline">\(R^2\)</span> for our model is about .06 (the multiple r-squared value). We interpret this as our model explains 6% of the variance in the level of perceived antisocial behaviour.</p></li>
</ol>
<p><br>
<br></p>
<hr />
</div>
<div id="activity-7-residuals" class="section level4">
<h4><span class="header-section-number">9.2.2.2</span> Activity 7: Residuals</h4>
<p>In the output above we saw something called <strong>residuals</strong>. The residuals are the differences between the observed values of <span class="math inline">\(Y\)</span> for each observation minus the predicted or expected values of <span class="math inline">\(Y\)</span>. In other words the distances between each point in the dataset and the regression line (see Figure 9.3).</p>
<p><br></p>
<div class="figure">
<img src="Images/residual.jpg" alt="Figure 9.3 Residuals" />
<p class="caption"><strong>Figure 9.3</strong> Residuals</p>
</div>
<p><br></p>
<p>Figure 9.3 shows the regression line, which is our predicted values, and then the black dots are the actual observed values. The distance between them is essentially the amount by which we were wrong, and all these distances between observed and predicted values are our residuals. Earlier, we were introduced to least squared estimation, and it essentially aims to reduce the average of the squares of all these distances: that is how it draws the line.</p>
<p>Why do we have residuals? The fact that the line is not a perfect representation of the many points makes sense. You cannot predict perfectly what the value of <span class="math inline">\(Y\)</span> is for every observation only by looking at its level of <span class="math inline">\(X\)</span>. Other relevant factors may not be taken into account by our model to predict the values of <span class="math inline">\(Y\)</span>. And then, of course, we have measurement error and other forms of noise. For example, if we use relative deprivation (<span class="math inline">\(X\)</span>) to predict homicide (<span class="math inline">\(Y\)</span>), residuals are inevitable because relative deprivation is not the only factor that can predict homicide – the distance reflects the predictive limitations of that particular model. In other words, how much variation is unexplained.</p>
<p>The regression line aims to improve that prediction. By knowing the values of <span class="math inline">\(X\)</span>, we can build a regression line that aims to get us closer to the actual values of <span class="math inline">\(Y\)</span> (Figure 9.4). A good model tries to maximise explained variation and minimise the magnitude of the residuals.</p>
<!--The regression line is only able to use information regarding $X$, so, consequently, there is bound to be some difference between our predicted level of $Y$, given our knowledge of $X$ (the regression line) and the actual level of $Y$ (the actual location of the points in the scatterplot).-->
<p>We revise our regression equation to represent each value of <span class="math inline">\(Y\)</span> (rather than the predicted value of <span class="math inline">\(Y\)</span>) that takes into account residuals:</p>
<p><br></p>
<p><span class="math inline">\(y = b_0 + b_1x + e\)</span></p>
<p><br></p>
<p><br></p>
<div class="figure">
<img src="Images/weight2.png" alt="Figure 9.4 R-squared" />
<p class="caption"><strong>Figure 9.4</strong> R-squared</p>
</div>
<p><br></p>
<p>We can use information from the residuals to produce a measure of <em>effect size</em>, of how good our model is in predicting variation in our dependent variable. This is <em><span class="math inline">\(R^2\)</span></em>.</p>
<p>Recall Activity 3 where we tried to guess levels of perceived antisocial behaviour. If we did not have any information about <span class="math inline">\(X\)</span>, which was age, our best bet for <span class="math inline">\(Y\)</span> would be the mean value of <span class="math inline">\(Y\)</span>.</p>
<p>The distance (squared differences) between that mean value of <span class="math inline">\(Y\)</span> and the observed values of <span class="math inline">\(Y\)</span> is what we call the <strong>total variation</strong> (total deviation in Figure 9.4). Total variation comprises two parts: explained and unexplained variation.</p>
<p>Explained variation is the the distance (again, squared differences) between the regression line and the mean value of <span class="math inline">\(Y\)</span>; this part is predictable from <span class="math inline">\(X\)</span>. In contrast, unexplained variation are the residuals, which is the distance between the regression line and the observed values of <span class="math inline">\(Y\)</span>.</p>
<!--The difference between the mean value of $Y$ and the predicted value of $Y$ (derived from the regression line) is how much better we are doing with our prediction by using information about $X$ (i.e., in our previous example it would be variation in Y that can be *explained* by knowing about resource deprivation). How much closer the regression line gets us to the observed values. -->
<p>To produce <em><span class="math inline">\(R^2\)</span></em>, we would contrast total variation and explained variation. The formula is as follows:</p>
<p><br></p>
<p><span class="math inline">\(R^2 = \dfrac{SSR}{SST} = \dfrac{\Sigma(\hat y_i - \bar y )^2}{\Sigma(y_i - \bar y )^2}\)</span></p>
<p><br></p>
<p>Whereby SSR is sum of squares regression and SST is sum of squares total. All this formula is doing is taking a ratio of the explained variation (SSR) by the total variation (SST). This gives us a measure of the <em>percentage of variation in <span class="math inline">\(Y\)</span> that is ‘explained’ by <span class="math inline">\(X\)</span></em>.</p>
<p><br>
<br></p>
<hr />
</div>
<div id="activity-8-multiple-regression" class="section level4">
<h4><span class="header-section-number">9.2.2.3</span> Activity 8: Multiple Regression</h4>
<p>At the start of the session, there was mention of one of the major drawbacks of looking at only bivariate relationships: these analyses did not consider other possible explanations.</p>
<p>It could be that you have a statistically significant relationship, but without considering other variables that may explain this relationship, your significant result could, in fact, be <em>spurious</em> – an association that should not be taken very seriously. For some ridiculous examples, click <a href="http://tylervigen.com/spurious-correlations">here</a>.</p>
<p>Multiple regression is a way for assessing the relevance of competing explanations. For example, we think there is a relationship between age and perceived antisocial behaviour levels, but we have not considered whether other reasons may provide a better explanation for these perceptions. Maybe previous victimisation or fear of crime may be better contenders.</p>
<p>The equation for multiple regression is similar to bivariate regression except it includes more coefficients:</p>
<p><br></p>
<p><span class="math display">\[y = b_0 + b_1x_i + b_2x_{ii} +... b_nx_n + e\]</span></p>
<p><br></p>
<p>For this example, we include additional variables to our model: sex (<code>sex</code>), previous victimisation in past 12 months (<code>bcsvictim</code>), and confidence in the police (<code>confx</code>). First, we check out these variables and tidy them as necessary:
<br></p>
<div class="sourceCode" id="cb542"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb542-1" title="1"><span class="co"># Checking out class of variables</span></a>
<a class="sourceLine" id="cb542-2" title="2"><span class="kw">class</span>(df<span class="op">$</span>confx) </a></code></pre></div>
<pre><code>## [1] &quot;numeric&quot;</code></pre>
<div class="sourceCode" id="cb544"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb544-1" title="1"><span class="kw">class</span>(df<span class="op">$</span>bcsvictim)</a></code></pre></div>
<pre><code>## [1] &quot;integer&quot;</code></pre>
<div class="sourceCode" id="cb546"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb546-1" title="1"><span class="co"># attach labels to bcsvictim and make ordering false </span></a>
<a class="sourceLine" id="cb546-2" title="2">df<span class="op">$</span>bcsvictim &lt;-<span class="st"> </span><span class="kw">factor</span>(df<span class="op">$</span>bcsvictim, <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;not victim&quot;</span>, <span class="st">&quot;Yes victim&quot;</span>), <span class="dt">ordered =</span> <span class="ot">FALSE</span>) </a>
<a class="sourceLine" id="cb546-3" title="3"></a>
<a class="sourceLine" id="cb546-4" title="4"><span class="co"># Sex</span></a>
<a class="sourceLine" id="cb546-5" title="5"><span class="kw">class</span>(df<span class="op">$</span>sex)</a></code></pre></div>
<pre><code>## [1] &quot;integer&quot;</code></pre>
<div class="sourceCode" id="cb548"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb548-1" title="1"><span class="kw">table</span>(df<span class="op">$</span>sex)</a></code></pre></div>
<pre><code>## 
##    1    2 
## 4064 4586</code></pre>
<div class="sourceCode" id="cb550"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb550-1" title="1"><span class="co"># Recode sex as dummy variable (0 and 1) where female is 0 and ordered is false</span></a>
<a class="sourceLine" id="cb550-2" title="2">df<span class="op">$</span>gender &lt;-<span class="st"> </span><span class="kw">recode</span>(df<span class="op">$</span>sex, <span class="st">`</span><span class="dt">1</span><span class="st">`</span> =<span class="st"> </span><span class="dv">1</span>, <span class="st">`</span><span class="dt">2</span><span class="st">`</span> =<span class="st"> </span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb550-3" title="3"></a>
<a class="sourceLine" id="cb550-4" title="4"><span class="co"># Change to factor</span></a>
<a class="sourceLine" id="cb550-5" title="5">df<span class="op">$</span>gender &lt;-<span class="st"> </span><span class="kw">factor</span>(df<span class="op">$</span>gender, <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;female&quot;</span>, <span class="st">&quot;male&quot;</span>), <span class="dt">ordered =</span> <span class="ot">FALSE</span>) </a>
<a class="sourceLine" id="cb550-6" title="6"></a>
<a class="sourceLine" id="cb550-7" title="7"><span class="co"># Double check values match original variable</span></a>
<a class="sourceLine" id="cb550-8" title="8"><span class="kw">table</span>(df<span class="op">$</span>gender)</a></code></pre></div>
<pre><code>## 
## female   male 
##   4586   4064</code></pre>
<p><br></p>
<p>Multiple regression is one way of checking the relevance of competing explanations. You could set up a regression model where you try to predict crime levels with a variable measuring broken windows and a variable measuring structural disadvantage. If, after controlling or adjusting for structural disadvantage, you see that the regression coefficient for broken windows is still statistically significant, you may be onto something, particularly if the estimated effect is still large.</p>
<p>If, however, the regression coefficient of your broken windows variable is no longer significant, then you may be tempted to think that perhaps opponents of Broken Windows Theory (Lesson 7, section 7.2.3.1) were onto something.</p>
<p>It could not be any easier to fit a multiple regression model. You simply modify the formula in the <code>lm()</code> function by adding the additional variables, separated with a <code>+</code>. Let us run this to predict antisocial behaviour with all our independent variables:
<br></p>
<div class="sourceCode" id="cb552"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb552-1" title="1"><span class="co"># Placing multiple regression into object called &#39;fit_2&#39;</span></a>
<a class="sourceLine" id="cb552-2" title="2">fit_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">lm</span>(antisocx <span class="op">~</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>gender <span class="op">+</span><span class="st"> </span>bcsvictim <span class="op">+</span><span class="st"> </span>confx, <span class="dt">data =</span> df)</a>
<a class="sourceLine" id="cb552-3" title="3"></a>
<a class="sourceLine" id="cb552-4" title="4"><span class="co"># Running the multiple regression</span></a>
<a class="sourceLine" id="cb552-5" title="5"><span class="kw">summary</span>(fit_<span class="dv">2</span>)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = antisocx ~ age + gender + bcsvictim + confx, data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.8854 -0.5684  0.1142  0.7195  2.5249 
## 
## Coefficients:
##                       Estimate Std. Error t value             Pr(&gt;|t|)
## (Intercept)         -0.5136682  0.0328072 -15.657 &lt; 0.0000000000000002
## age                  0.0104148  0.0005699  18.276 &lt; 0.0000000000000002
## gendermale           0.1003834  0.0207701   4.833           0.00000137
## bcsvictimYes victim -0.4310943  0.0290209 -14.855 &lt; 0.0000000000000002
## confx                0.2038268  0.0106002  19.229 &lt; 0.0000000000000002
## 
## Residual standard error: 0.9361 on 8151 degrees of freedom
##   (494 observations deleted due to missingness)
## Multiple R-squared:  0.1247, Adjusted R-squared:  0.1243 
## F-statistic: 290.3 on 4 and 8151 DF,  p-value: &lt; 0.00000000000000022</code></pre>
<p><br></p>
<p>We return to those four points to focus on when confronted with a regression output:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Beta coefficients</strong>: The usual interpretation is to state that <span class="math inline">\(Y\)</span> changes for every one-unit increase in <span class="math inline">\(X\)</span> <em>when the other variables in the model are held constant</em> (or are ‘controlled for’). Therefore, a 0.01 increase in level of perceived antisocial behaviour in the neighbourhood is associated with a one-year increase in age, for example. This is similar to the bivariate regression. For categorical variables, however, the interpretation is different. Notice how these variables have an additional label attached to them in the output. For example, <code>gender</code> includes ‘male’ and <code>bcsvictim</code> includes ‘yes victim’. Labels that do not appear next to the name of the categorical variable in the model are called the <em>reference category</em>. This category is the lowest value. You can select your reference category based on the modal category or if it is a category in which you are least interested by recoding the variable. We interpret them as the following: males have a perceived antisocial behaviour level that averages 0.10 units higher than that of females; victims have a perceived antisocial behaviour level that averages .43 units lower than that of non-victims. A one-point score increase for confidence in neighbourhood police is related to a .20 increase in level of perceived antisocial behaviour. For each interpretation, covariates in the model are controlled for.</p></li>
<li><p><strong>P-value</strong>: All coefficients are statistically significant, meaning that we can reject the null hypothesis that no differences exist on perceived antisocial behaviour.</p></li>
<li><p><strong>F-statistic</strong>: Now that we have more than one IV in our model, we evaluate whether all of the regression coefficients are zero. The F-statistic is below the conventional significance level of <span class="math inline">\(\alpha\)</span> = 0.05, so at least one of our IVs is related to our DV. You will also notice t-values for each IV. These are testing whether each of the IVs is associated with the DV when controlling for covariates in the model.</p></li>
<li><p><strong>(Multiple) <span class="math inline">\(R^2\)</span></strong>: The <span class="math inline">\(R^2\)</span> is higher than that of our previous bivariate regression model. It will, however, inevitably increase whenever new variables are added to the model, regardless if they are only weakly related to the DV. Having said this, the new <span class="math inline">\(R^2\)</span> value suggests that the addition of the three new variables improves our previous model.</p></li>
</ol>
<p><br>
<br></p>
<hr />
</div>
<div id="activity-9-presenting-the-results" class="section level4">
<h4><span class="header-section-number">9.2.2.4</span> Activity 9: Presenting the results</h4>
<p>If you would like to professionally present your results, you can use the function <code>tab_model()</code> from the <code>sjPlot</code> package:
<br></p>
<div class="sourceCode" id="cb554"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb554-1" title="1"><span class="kw">tab_model</span>(fit_<span class="dv">2</span>)</a></code></pre></div>
<table style="border-collapse:collapse; border:none;">
<tr>
<th style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; ">
 
</th>
<th colspan="3" style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; ">
antisocx
</th>
</tr>
<tr>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; ">
Predictors
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
Estimates
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
CI
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
p
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
(Intercept)
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
-0.51
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
-0.58 – -0.45
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
age
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.01
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.01 – 0.01
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
gender [male]
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.10
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.06 – 0.14
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
bcsvictim [Yes victim]
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
-0.43
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
-0.49 – -0.37
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
confx
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.20
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.18 – 0.22
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;">
Observations
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;" colspan="3">
8156
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
R<sup>2</sup> / R<sup>2</sup> adjusted
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;" colspan="3">
0.125 / 0.124
</td>
</tr>
</table>
<div class="sourceCode" id="cb555"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb555-1" title="1"><span class="co"># Change name of DV</span></a>
<a class="sourceLine" id="cb555-2" title="2"><span class="kw">tab_model</span>(fit_<span class="dv">2</span>, <span class="dt">dv.labels =</span> <span class="st">&quot;Perceived Antisocial Behaviour&quot;</span>)</a></code></pre></div>
<table style="border-collapse:collapse; border:none;">
<tr>
<th style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; ">
 
</th>
<th colspan="3" style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; ">
Perceived Antisocial Behaviour
</th>
</tr>
<tr>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; ">
Predictors
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
Estimates
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
CI
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
p
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
(Intercept)
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
-0.51
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
-0.58 – -0.45
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
age
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.01
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.01 – 0.01
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
gender [male]
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.10
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.06 – 0.14
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
bcsvictim [Yes victim]
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
-0.43
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
-0.49 – -0.37
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
confx
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.20
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.18 – 0.22
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;">
Observations
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;" colspan="3">
8156
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
R<sup>2</sup> / R<sup>2</sup> adjusted
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;" colspan="3">
0.125 / 0.124
</td>
</tr>
</table>
<div class="sourceCode" id="cb556"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb556-1" title="1"><span class="co"># Change name of IVs</span></a>
<a class="sourceLine" id="cb556-2" title="2"><span class="kw">tab_model</span>(fit_<span class="dv">2</span>, <span class="dt">pred.labels =</span> <span class="kw">c</span>(<span class="st">&quot;(Intercept)&quot;</span>, <span class="st">&quot;Age&quot;</span>, <span class="st">&quot;Sex&quot;</span>, <span class="st">&quot;Victimisation&quot;</span>, <span class="st">&quot;Confidence in Police&quot;</span>), <span class="dt">dv.labels =</span> <span class="st">&quot;Perceived Antisocial Behaviour&quot;</span>)</a></code></pre></div>
<table style="border-collapse:collapse; border:none;">
<tr>
<th style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; ">
 
</th>
<th colspan="3" style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; ">
Perceived Antisocial Behaviour
</th>
</tr>
<tr>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; ">
Predictors
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
Estimates
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
CI
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
p
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
(Intercept)
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
-0.51
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
-0.58 – -0.45
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
Age
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.01
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.01 – 0.01
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
Sex
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.10
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.06 – 0.14
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
Victimisation
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
-0.43
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
-0.49 – -0.37
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
Confidence in Police
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.20
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.18 – 0.22
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;">
Observations
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;" colspan="3">
8156
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
R<sup>2</sup> / R<sup>2</sup> adjusted
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;" colspan="3">
0.125 / 0.124
</td>
</tr>
</table>
<p><br></p>
<p>You may also like to present your results visually with a plot. For this, you can use the <code>plot_model()</code> function of the <code>sjPlot</code> package, which makes it easier to produce these sorts of plots. You can find a more detailed tutorial about this function <a href="https://strengejacke.wordpress.com/2017/10/23/one-function-to-rule-them-all-visualization-of-regression-models-in-rstats-w-sjplot/">here</a>.
<br></p>
<div class="sourceCode" id="cb557"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb557-1" title="1"><span class="kw">plot_model</span>(fit_<span class="dv">2</span>)</a></code></pre></div>
<p><img src="09-regression_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p><br></p>
<p>What you see plotted is the point estimates (the circles), the confidence intervals around those estimates (the tiny ‘whiskers’- the longer the line, the less precise the estimate), and the colours represent whether the effect is negative (red) or positive (blue).</p>
<p>There are other packages that also provide similar functionality, like the <code>dotwhisker</code> package that you may want to explore. (More details <a href="https://cran.r-project.org/web/packages/dotwhisker/vignettes/dotwhisker-vignette.html">here</a>.)</p>
<p><br>
<br></p>
<hr />
</div>
</div>
<div id="logistic-regression" class="section level3">
<h3><span class="header-section-number">9.2.3</span> Logistic Regression</h3>
<p>What if our dependent variable (DV) is categorical and binary? This is where <strong>logistic regression</strong>, a technique belonging to a family of techniques called <strong>generalized linear models</strong>, is appropriate. Those assumptions of linearity and normality are not issues because this technique is not based on the normal distribution.</p>
<p>In criminology, it is often the case that our DV has only two categories. For example, victim or not a victim; arrested or not arrested; crime or no crime. This form of regression models the probability of belonging to one of the levels of the binary outcome.</p>
<p>For this example, we are interested in the relationship between race and receiving harsher treatment for possession of marijuana. We use the dataset <code>Arrests</code> from the <code>effects</code> package. This data includes information on police treatment of individuals arrested for possession of marijuana in Toronto, Canada.</p>
<p>Our DV is whether the arrestee was released with summons; if they were not, they received harsh treatment such as being brought to the police station or held for bail:
<br></p>
<div class="sourceCode" id="cb558"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb558-1" title="1"><span class="kw">data</span>(Arrests, <span class="dt">package=</span><span class="st">&quot;effects&quot;</span>) <span class="co"># It will tell you that the dataset are not found, but it is there</span></a>
<a class="sourceLine" id="cb558-2" title="2"></a>
<a class="sourceLine" id="cb558-3" title="3"><span class="co"># Checking out our DV</span></a>
<a class="sourceLine" id="cb558-4" title="4"><span class="kw">table</span>(Arrests<span class="op">$</span>released)</a></code></pre></div>
<pre><code>## 
##   No  Yes 
##  892 4334</code></pre>
<div class="sourceCode" id="cb560"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb560-1" title="1"><span class="co"># Checking out the order of the levels in the DV</span></a>
<a class="sourceLine" id="cb560-2" title="2"><span class="kw">attributes</span>(Arrests<span class="op">$</span>released)</a></code></pre></div>
<pre><code>## $levels
## [1] &quot;No&quot;  &quot;Yes&quot;
## 
## $class
## [1] &quot;factor&quot;</code></pre>
<p><br>
<br></p>
<div id="activity-10-preparing-your-dependent-variable" class="section level4">
<h4><span class="header-section-number">9.2.3.1</span> Activity 10: Preparing your dependent variable</h4>
<p>Logistic regression will predict probabilities associated with the level whose first letter is further in the alphabet — this would be ‘yes’, the respondent was released with a summons. But this is not our level of interest. We will need to reorder the levels in the DV using the <code>relevel</code> function:
<br></p>
<div class="sourceCode" id="cb562"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb562-1" title="1"><span class="co"># Reverse the order</span></a>
<a class="sourceLine" id="cb562-2" title="2"><span class="co">#Rename the levels so that it is clear we now mean &#39;yes&#39; to harsh treatment</span></a>
<a class="sourceLine" id="cb562-3" title="3">Arrests<span class="op">$</span>harsher &lt;-<span class="st"> </span><span class="kw">relevel</span>(Arrests<span class="op">$</span>released, <span class="st">&quot;Yes&quot;</span>)</a>
<a class="sourceLine" id="cb562-4" title="4">Arrests<span class="op">$</span>harsher &lt;-<span class="st"> </span><span class="kw">factor</span>(Arrests<span class="op">$</span>harsher, <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;No&quot;</span>, <span class="st">&quot;Yes&quot;</span>), <span class="dt">ordered =</span> <span class="ot">TRUE</span>) </a>
<a class="sourceLine" id="cb562-5" title="5"></a>
<a class="sourceLine" id="cb562-6" title="6"><span class="co">#  Check that it matches the original variable ‘released’ but in reverse order</span></a>
<a class="sourceLine" id="cb562-7" title="7"><span class="kw">table</span>(Arrests<span class="op">$</span>harsher)</a></code></pre></div>
<pre><code>## 
##   No  Yes 
## 4334  892</code></pre>
<div class="sourceCode" id="cb564"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb564-1" title="1"><span class="kw">table</span>(Arrests<span class="op">$</span>released)</a></code></pre></div>
<pre><code>## 
##   No  Yes 
##  892 4334</code></pre>
<div class="sourceCode" id="cb566"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb566-1" title="1"><span class="co"># We will also reverse the levels in our race variable so that ‘white’ is ‘0’</span></a>
<a class="sourceLine" id="cb566-2" title="2"><span class="kw">table</span>(Arrests<span class="op">$</span>colour)</a></code></pre></div>
<pre><code>## 
## Black White 
##  1288  3938</code></pre>
<div class="sourceCode" id="cb568"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb568-1" title="1">Arrests<span class="op">$</span>race &lt;-<span class="st"> </span>Arrests<span class="op">$</span>colour</a>
<a class="sourceLine" id="cb568-2" title="2">Arrests<span class="op">$</span>race &lt;-<span class="st"> </span><span class="kw">relevel</span>(Arrests<span class="op">$</span>race, <span class="st">&quot;White&quot;</span>)</a>
<a class="sourceLine" id="cb568-3" title="3"></a>
<a class="sourceLine" id="cb568-4" title="4"><span class="co"># Check to see coding has changed</span></a>
<a class="sourceLine" id="cb568-5" title="5"><span class="kw">table</span>(Arrests<span class="op">$</span>race)</a></code></pre></div>
<pre><code>## 
## White Black 
##  3938  1288</code></pre>
<p><br>
<br></p>
<hr />
</div>
<div id="activity-11-fitting-and-interpreting-a-logistic-regression-model" class="section level4">
<h4><span class="header-section-number">9.2.3.2</span> Activity 11: Fitting and interpreting a logistic regression model</h4>
<p>Now that we have arranged the codes of our variables of interest, we fit the logistic regression model using the <code>glm ()</code> function and specify a logit model (<code>family=”binomial”</code>). We include three other variables in the model to see to what extent race seems to matter in harsher police treatment even when we control for or consider sex, employment, and previous police contacts:
<br></p>
<div class="sourceCode" id="cb570"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb570-1" title="1">fit_<span class="dv">3</span> &lt;-<span class="st"> </span><span class="kw">glm</span>(harsher <span class="op">~</span><span class="st"> </span>race <span class="op">+</span><span class="st"> </span>checks <span class="op">+</span><span class="st"> </span>sex <span class="op">+</span><span class="st"> </span>employed, <span class="dt">data=</span>Arrests, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)</a>
<a class="sourceLine" id="cb570-2" title="2"></a>
<a class="sourceLine" id="cb570-3" title="3"><span class="kw">summary</span>(fit_<span class="dv">3</span>)</a></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = harsher ~ race + checks + sex + employed, family = &quot;binomial&quot;, 
##     data = Arrests)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.5226  -0.6156  -0.4407  -0.3711   2.3449  
## 
## Coefficients:
##             Estimate Std. Error z value             Pr(&gt;|z|)
## (Intercept) -1.90346    0.15999 -11.898 &lt; 0.0000000000000002
## raceBlack    0.49608    0.08264   6.003        0.00000000194
## checks       0.35796    0.02580  13.875 &lt; 0.0000000000000002
## sexMale      0.04215    0.14965   0.282                0.778
## employedYes -0.77973    0.08386  -9.298 &lt; 0.0000000000000002
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 4776.3  on 5225  degrees of freedom
## Residual deviance: 4330.7  on 5221  degrees of freedom
## AIC: 4340.7
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p><br></p>
<p>The output is similar to the one produced for linear regression, although there are some differences such as the Akaike information criterion (AIC) and the z-statistic, which is a similar idea to the t-test in linear regression. The z-statistic is a test of statistical significance and assesses whether each variable in the model is associated with the DV.</p>
<p>All IVs are significantly associated with harsher police treatment except for sex. We see that for every one unit increase in the number of previous police contacts, the log odds of receiving harsher treatment (versus being released on summons) increases by 0.36, adjusting for the other variables in the model.</p>
<p>Observe that categorical variables have a label next to them, too, in the output. For example, sex has ‘male’ next to it; race has ‘black’ next to it; employed has ‘yes’ next to it. These are the categories you are comparing to the reference category (whichever level is coded ‘0’). So, being black increases the log odds of receiving harsh treatment by 0.49 compared to being white, while being employed decreases the log odds of harsh treatment by 0.77 compared to being unemployed.</p>
<p>This is good, but most people find interpreting log odds, well, odd – it is not very intuitive. It is common to use the <strong>odds ratio</strong> (OR) to interpret logistic regression. We convert these values into ORs:
<br></p>
<div class="sourceCode" id="cb572"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb572-1" title="1"><span class="kw">exp</span>(<span class="kw">coef</span>(fit_<span class="dv">3</span>))</a></code></pre></div>
<pre><code>## (Intercept)   raceBlack      checks     sexMale employedYes 
##   0.1490516   1.6422658   1.4304108   1.0430528   0.4585312</code></pre>
<div class="sourceCode" id="cb574"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb574-1" title="1"><span class="co"># We can add confidence intervals</span></a>
<a class="sourceLine" id="cb574-2" title="2"><span class="kw">exp</span>(<span class="kw">cbind</span>(<span class="dt">OR =</span> <span class="kw">coef</span>(fit_<span class="dv">3</span>), <span class="kw">confint</span>(fit_<span class="dv">3</span>)))</a></code></pre></div>
<pre><code>## Waiting for profiling to be done...</code></pre>
<pre><code>##                    OR     2.5 %    97.5 %
## (Intercept) 0.1490516 0.1081600 0.2026495
## raceBlack   1.6422658 1.3957633 1.9298763
## checks      1.4304108 1.3601419 1.5049266
## sexMale     1.0430528 0.7830594 1.4090622
## employedYes 0.4585312 0.3892103 0.5407210</code></pre>
<div class="sourceCode" id="cb577"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb577-1" title="1"><span class="co"># Another way of getting the results, which uses less typing, is to use the Logit() function from the lessr package</span></a>
<a class="sourceLine" id="cb577-2" title="2"></a>
<a class="sourceLine" id="cb577-3" title="3"><span class="kw">Logit</span>(harsher <span class="op">~</span><span class="st"> </span>checks <span class="op">+</span><span class="st"> </span>colour <span class="op">+</span><span class="st"> </span>sex <span class="op">+</span><span class="st"> </span>employed, <span class="dt">data=</span>Arrests, <span class="dt">brief=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## 
## Response Variable:   harsher
## Predictor Variable 1:  checks
## Predictor Variable 2:  colour
## Predictor Variable 3:  sex
## Predictor Variable 4:  employed
## 
## Number of cases (rows) of data:  5226 
## Number of cases retained for analysis:  5226 
## 
## 
## 
##    BASIC ANALYSIS 
## 
## Model Coefficients
## 
##              Estimate    Std Err  z-value  p-value   Lower 95%   Upper 95%
## (Intercept)   -1.4074     0.1724   -8.162    0.000     -1.7453     -1.0694 
##      checks    0.3580     0.0258   13.875    0.000      0.3074      0.4085 
## colourWhite   -0.4961     0.0826   -6.003    0.000     -0.6580     -0.3341 
##     sexMale    0.0422     0.1496    0.282    0.778     -0.2511      0.3355 
## employedYes   -0.7797     0.0839   -9.298    0.000     -0.9441     -0.6154 
## 
## 
## Odds ratios and confidence intervals
## 
##              Odds Ratio   Lower 95%   Upper 95%
## (Intercept)      0.2448      0.1746      0.3432 
##      checks      1.4304      1.3599      1.5046 
## colourWhite      0.6089      0.5179      0.7160 
##     sexMale      1.0431      0.7779      1.3986 
## employedYes      0.4585      0.3890      0.5404 
## 
## 
## Model Fit
## 
##     Null deviance: 4776.258 on 5225 degrees of freedom
## Residual deviance: 4330.699 on 5221 degrees of freedom
## 
## AIC: 4340.699 
## 
## Number of iterations to convergence: 5 
## 
## 
## 
## 
## &gt;&gt;&gt; Note:  colour is not a numeric variable.
## 
## 
## 
## &gt;&gt;&gt; Note:  sex is not a numeric variable.
## 
## 
## 
## &gt;&gt;&gt; Note:  employed is not a numeric variable.
## 
## Collinearity
## 
## 
## &gt;&gt;&gt; No collinearity analysis because not all variables are numeric.</code></pre>
<p><br></p>
<p>If the odds ratio is greater than 1, it indicates that the odds of receiving harsh treatment increases when the independent variable, too, increases. Previous police contacts increase the odds of harsher treatment by 43% and being black increases the odds of harsher treatment by 64% (all the while controlling for covariates in the model). Employment, however, has an odds ratio of 0.45. If the OR is between 0 and 1, it is considered a negative relationship.</p>
<p>Referring to the confidence intervals is also helpful: if 1 is within the interval, the result is statistically non-significant. Take the 95% CI of sex, for example.</p>
<p>For both types of regression, it is inappropriate to make comparisons between IVs. You cannot directly compare the coefficients of one IV to the other, unless they use the same metric. For more ways to interpret ORs in logistic regression, refer to the <a href="https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/">UCLA Institute for Digital Research and Education</a></p>
<p><br>
<br></p>
<hr />
</div>
<div id="model-fit" class="section level4">
<h4><span class="header-section-number">9.2.3.3</span> Model Fit</h4>
<p>How well is the fit of our logistic regression model? In order words, how well does our IVs collectively predict the DV?</p>
<p>Recall from OLS regression that we assessed how well our overall model predicts the DV by observing the <em>F-statistic</em> and, especially, the <em><span class="math inline">\(R^2\)</span></em>. This is not the case for logistic regression. The reason is, in our familiar linear regression, we understand residual variation (related to that regression line) through the error sum of squares.</p>
<p>In logistic regression, there is not one way to understand residual variation, but several, because of its binary outcome. This results in different ways of understanding model fit: (1) quantitative prediction, focused on how close the prediction is to being correct; (2) qualitative prediction, focused on whether the prediction is correct or incorrect; or (3) both. We only go through the first one.</p>
</div>
<div id="activity-12-quantitative-prediction" class="section level4">
<h4><span class="header-section-number">9.2.3.4</span> Activity 12: Quantitative Prediction</h4>
<p>A common measure for evaluating model fit is the <strong>deviance</strong>, also known as -2LL. It measures how accurate the prediction is by multiplying the log likelihood statistic by -2. This <em>log-likelihood statistic</em> measures how much unexplained variance remains after the model is fitted, whereby larger values reflect poorer fit. Larger values of -2LL, similarly, indicate worse prediction.</p>
<p>The logistic regression model can be estimated using an approach of probability called <em>maximum likelihood estimation</em>, which calculates the probability of obtaining a certain result given the IVs in the model. It does so by minimising that -2LL. This is like what OLS regression does with the error sum of squares – tries to minimise it. Thus, -2LL is to logistic regression as the error sum of squares is to OLS regression.</p>
<p>When we re-run our model, we find that -2LL is in the form of <em>null</em> and <em>residual</em> deviances towards the bottom of the output:
<br></p>
<div class="sourceCode" id="cb579"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb579-1" title="1"><span class="kw">Logit</span>(harsher <span class="op">~</span><span class="st"> </span>checks <span class="op">+</span><span class="st"> </span>colour <span class="op">+</span><span class="st"> </span>sex <span class="op">+</span><span class="st"> </span>employed, <span class="dt">data=</span>Arrests, <span class="dt">brief=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## 
## Response Variable:   harsher
## Predictor Variable 1:  checks
## Predictor Variable 2:  colour
## Predictor Variable 3:  sex
## Predictor Variable 4:  employed
## 
## Number of cases (rows) of data:  5226 
## Number of cases retained for analysis:  5226 
## 
## 
## 
##    BASIC ANALYSIS 
## 
## Model Coefficients
## 
##              Estimate    Std Err  z-value  p-value   Lower 95%   Upper 95%
## (Intercept)   -1.4074     0.1724   -8.162    0.000     -1.7453     -1.0694 
##      checks    0.3580     0.0258   13.875    0.000      0.3074      0.4085 
## colourWhite   -0.4961     0.0826   -6.003    0.000     -0.6580     -0.3341 
##     sexMale    0.0422     0.1496    0.282    0.778     -0.2511      0.3355 
## employedYes   -0.7797     0.0839   -9.298    0.000     -0.9441     -0.6154 
## 
## 
## Odds ratios and confidence intervals
## 
##              Odds Ratio   Lower 95%   Upper 95%
## (Intercept)      0.2448      0.1746      0.3432 
##      checks      1.4304      1.3599      1.5046 
## colourWhite      0.6089      0.5179      0.7160 
##     sexMale      1.0431      0.7779      1.3986 
## employedYes      0.4585      0.3890      0.5404 
## 
## 
## Model Fit
## 
##     Null deviance: 4776.258 on 5225 degrees of freedom
## Residual deviance: 4330.699 on 5221 degrees of freedom
## 
## AIC: 4340.699 
## 
## Number of iterations to convergence: 5 
## 
## 
## 
## 
## &gt;&gt;&gt; Note:  colour is not a numeric variable.
## 
## 
## 
## &gt;&gt;&gt; Note:  sex is not a numeric variable.
## 
## 
## 
## &gt;&gt;&gt; Note:  employed is not a numeric variable.
## 
## Collinearity
## 
## 
## &gt;&gt;&gt; No collinearity analysis because not all variables are numeric.</code></pre>
<p><br></p>
<p>The null deviance is the value of -2LL for a model without any IVs, whereas the residual deviance is the value of -2LL for our present model. We test to see if the null hypothesis, that the regression coefficients (IVs) equal zero, is true. We do so by calculating the <em>model chi-squared</em>. This is the difference between both deviances:
<br></p>
<div class="sourceCode" id="cb581"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb581-1" title="1"><span class="co"># All the information we need to calculate the model chi-squared is already in our model object</span></a>
<a class="sourceLine" id="cb581-2" title="2"><span class="kw">names</span>(fit_<span class="dv">3</span>)</a></code></pre></div>
<pre><code>##  [1] &quot;coefficients&quot;      &quot;residuals&quot;         &quot;fitted.values&quot;    
##  [4] &quot;effects&quot;           &quot;R&quot;                 &quot;rank&quot;             
##  [7] &quot;qr&quot;                &quot;family&quot;            &quot;linear.predictors&quot;
## [10] &quot;deviance&quot;          &quot;aic&quot;               &quot;null.deviance&quot;    
## [13] &quot;iter&quot;              &quot;weights&quot;           &quot;prior.weights&quot;    
## [16] &quot;df.residual&quot;       &quot;df.null&quot;           &quot;y&quot;                
## [19] &quot;converged&quot;         &quot;boundary&quot;          &quot;model&quot;            
## [22] &quot;call&quot;              &quot;formula&quot;           &quot;terms&quot;            
## [25] &quot;data&quot;              &quot;offset&quot;            &quot;control&quot;          
## [28] &quot;method&quot;            &quot;contrasts&quot;         &quot;xlevels&quot;</code></pre>
<div class="sourceCode" id="cb583"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb583-1" title="1"><span class="co"># We now calculate said chi-squared by taking the difference of the deviances </span></a>
<a class="sourceLine" id="cb583-2" title="2"><span class="kw">with</span>(fit_<span class="dv">3</span>, null.deviance <span class="op">-</span><span class="st"> </span>deviance)</a></code></pre></div>
<pre><code>## [1] 445.5594</code></pre>
<p><br></p>
<p>Our obtained value is: 445.56. We do not know, though, how big or small this value is to determine how predictive our model is. We will need a test of statistical significance:
<br></p>
<div class="sourceCode" id="cb585"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb585-1" title="1"><span class="co"># Obtaining the p-value</span></a>
<a class="sourceLine" id="cb585-2" title="2"><span class="kw">with</span>(fit_<span class="dv">3</span>, <span class="kw">pchisq</span>(null.deviance <span class="op">-</span><span class="st"> </span>deviance, df.null <span class="op">-</span><span class="st"> </span>df.residual, <span class="dt">lower.tail =</span> <span class="ot">FALSE</span>))</a></code></pre></div>
<pre><code>## [1] 3.961177e-95</code></pre>
<div class="sourceCode" id="cb587"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb587-1" title="1"><span class="co"># Incorporating previous calculation of difference between deviances</span></a>
<a class="sourceLine" id="cb587-2" title="2"><span class="co"># df.null - df.residual: calculating the degrees of freedom (should equal the number of IVs in the model)</span></a></code></pre></div>
<p>The p-value is smaller than <span class="math inline">\(\alpha\)</span> = 0.05, so we conclude that our model is a better fit than a model with no IVs (or predictors).</p>
<p>In addition, <a href="https://methods.sagepub.com/book/logistic-regression-from-introductory-to-advanced-concepts-and-applications">Professor Scott Menard (2010)</a> suggests calculating the <em>likelihood ratio <span class="math inline">\(R^2\)</span></em>, also known as the Hosmer/Lemeshow <span class="math inline">\(R^2\)</span>, by taking the difference of the previous deviances and dividing it by the null deviance.</p>
<p>The likelihood ratio <span class="math inline">\(R^2\)</span> tells you the extent to which including your IVs in your model reduces the variation, or error, as measured by the null deviance. The value can range from 0 to 1, where higher values mean better predictive accuracy:
<br></p>
<div class="sourceCode" id="cb588"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb588-1" title="1"><span class="co">#Likelihood ratio R2</span></a>
<a class="sourceLine" id="cb588-2" title="2"><span class="kw">with</span>(fit_<span class="dv">3</span>, (null.deviance <span class="op">-</span><span class="st"> </span>deviance)<span class="op">/</span>null.deviance)</a></code></pre></div>
<pre><code>## [1] 0.09328629</code></pre>
<p><br></p>
<p>This particular <span class="math inline">\(R^2\)</span> is considered a ‘pseudo- <span class="math inline">\(R^2\)</span>’ as it is not the same as the one obtained in OLS/ linear regression. There are many kinds of pseudo ones but Menard recommended the likelihood ratio one because (1) of its conceptual similarity to the <span class="math inline">\(R^2\)</span> in OLS regression; (2) it is not easily swayed by the base rate (the number of cases that have the charateristic of interest); (3) it is easy to interpret; and (4) it can also be used in other generalised linear models, such as ones with categorical DVs with more than two levels.</p>
<p><br>
<br></p>
<hr />
</div>
</div>
</div>
<div id="summary-8" class="section level2">
<h2><span class="header-section-number">9.3</span> SUMMARY</h2>
<p>Regression allows us to look at relationships between variables while including other variables, called <strong>covariates</strong>, in the model. For OLS regression, assumptions must be met before embarking on fitting this model, and has to do with building the <strong>regression line</strong> whereby the line starts at the <strong>intercept</strong> and its angle is determined by the <strong>slope</strong>. <strong>Residuals</strong> form the equation to determine the <span class="math inline">\(R^2\)</span> and is the error of the model. When the DV is binary, logistic regression is appropriate. The <strong>odds ratio</strong> is an easier way to interpret logistic regression. We assess model fit through quantitative prediction measures of deviance, and it is recommended to use the likelihood ratio <span class="math inline">\(R^2\)</span> instead of other pseudo <span class="math inline">\(R^2s\)</span></p>
<p><br>
<br></p>
<p>Homework time!</p>
<p><br></p>
<p><br></p>
<p><br></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="more-on-effect-sizes.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/09-regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Modelling-Crime-Data-2021.pdf", "Modelling-Crime-Data-2021.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
